\documentclass{sbc2019}%

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[misc,geometry]{ifsym}
\usepackage{fontspec}
\usepackage{fontawesome}
\usepackage{academicons}
\usepackage{color}
\usepackage{hyperref}
\usepackage{aas_macros}
\usepackage[bottom]{footmisc}
\usepackage{supertabular}
\usepackage{afterpage}
\usepackage{url}
\usepackage{underscore}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{booktabs}

% Siglas
\usepackage[acronym]{glossaries-extra}
\input{acronyms}

\setcitestyle{square}

\definecolor{orcidlogo}{rgb}{0.37,0.48,0.13}
\definecolor{unilogo}{rgb}{0.16, 0.26, 0.58}
\definecolor{maillogo}{rgb}{0.58, 0.16, 0.26}
\definecolor{darkblue}{rgb}{0.0,0.0,0.0}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
%\hypersetup{colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue}

% Macro TODO usada no rascunho. Substituir ou remover na submissao final.
\newcommand{\TODO}[1]{#1}

%%%%%%% IMPORTANTE: Desabilitamos os hyperlinks por padrao com esta linha,
% para evitar o erro "\pdfendlink ended up in different nesting level" durante a escrita.
%\hypersetup{draft}

\jid{JISA}
\jtitle{Journal of Internet Services and Applications, 2026, 17:1, }
\doi{10.5753/jisa.2026.XXXXXX}
\copyrightstatement{This work is licensed under a Creative Commons Attribution 4.0 International License}
\jyear{2026}



\title[Preenchimento de Lacunas Guiado por Entropia em Imagens de Satélite]{Entropia de Shannon Local como Preditor da Qualidade do Preenchimento Clássico de Lacunas em Imagens Multiespectrais de Satélite}


\author[Pires et al. 2026]{
\affil{\textbf{Emanuel \\'Avila Pires}\textsuperscript{$\ast$}~\href{https://orcid.org/0000-0000-0000-0000}{\textcolor{orcidlogo}{\aiOrcid}}~\textcolor{blue}{\faEnvelopeO}~~[~\textbf{Universidade Federal do Cear\\'a}~|\href{mailto:emanuel.pires@alu.ufc.br}{~\textbf{\textit{emanuel.pires@alu.ufc.br}}}~]}

\affil{\textbf{\TODO{Autor Dois}}~\href{https://orcid.org/0000-0000-0000-0000}{\textcolor{orcidlogo}{\aiOrcid}}~~[~\textbf{Universidade Federal do Cear\\'a}~|\href{mailto:author2@ufc.br}{~\textbf{\textit{author2@ufc.br}}}~]}

}
% * Autor correspondente

\begin{document}

\begin{frontmatter}
\maketitle

\begin{mail}
PPGETI, Universidade Federal do Cear\'a, Fortaleza, Cear\'a, Brasil.
\textsuperscript{$\ast$}Autor correspondente.
\end{mail}

\begin{dates}
\small{\textbf{Recebido:} DD Mês AAAA~~~$\bullet$~~~\textbf{Aceito:} DD Mês AAAA~~~$\bullet$~~~\textbf{Publicado:} DD Mês AAAA}
%Lista completa de informações dos autores disponível ao final do artigo.
\end{dates}


\begin{abstract}
\textbf{Resumo}
\noindent Estudamos o preenchimento de lacunas em imagens de satélite e
quantificamos como a complexidade de cena afeta a qualidade de reconstrução.
Utilizando 15 métodos clássicos de interpolação (e modelos profundos
selecionados para comparação), avaliamos PSNR/SSIM/RMSE/SAM em diferentes
níveis de ruído, sensores e escalas de entropia. Análises de Spearman e
regressão robusta revelam uma relação negativa consistente entre a entropia de
Shannon local e o PSNR, indicando que maior complexidade de textura degrada a
qualidade em todos os métodos. A autocorrelação espacial (I de Moran e LISA)
revela agrupamentos estruturados de erros que se alinham com regiões de alta
entropia. Os resultados fornecem orientações práticas para a seleção de métodos
e benchmarking sob diferentes complexidades de cena.

\end{abstract}

\begin{keywords}
imagens de satélite; preenchimento de lacunas; entropia de Shannon;
interpolação espacial; avaliação de qualidade de imagem; sensoriamento remoto;
sensoriamento compressivo; autocorrelação espacial.
\end{keywords}

%\begin{license}
%Publicado sob a Licença Pública Internacional Creative Commons Atribuição 4.0 (CC BY 4.0)
%\end{license}

\end{frontmatter}

% ============================================================
\section{Introdução}
\label{sec:introduction}

O sensoriamento remoto por satélite fornece uma base observacional indispensável
para monitoramento ambiental, agricultura, planejamento urbano e ciência do
clima. Entretanto, mesmo com as constelações modernas (Sentinel-2 com revisita
de 5 dias, Landsat com 16 dias), uma fração substancial das cenas adquiridas é
corrompida por cobertura de nuvens, neblina, reflectância de neve, saturação de
sensor ou falhas sistemáticas na linha de varredura
.

Em regiões tropicais, as nuvens podem obscurecer mais de $50\%$ das aquisições
anuais , tornando a análise de séries temporais
inviável sem um eficaz \emph{preenchimento de lacunas}. Dezenas de métodos foram
propostos nas últimas duas décadas, desde interpolação espacial clássica até
decomposição espectral , inpainting baseado em
recortes , sensoriamento compressivo
, e, mais recentemente, aprendizado profundo
.

Apesar dessa diversidade, os profissionais carecem de orientações claras sobre
\emph{quando} aplicar cada método. Uma suposição implícita comum é que cenas
espacialmente suaves (baixa complexidade) são mais fáceis de reconstruir,
enquanto cenas ricas em textura -- paisagens urbanas, florestas heterogêneas,
costas litorâneas -- representam maiores desafios. Essa intuição não foi
rigorosamente quantificada ao longo de uma ampla variedade de métodos, sensores
e condições experimentais controladas.

\medskip
\noindent\textbf{Lacuna de pesquisa.}
Os estudos de benchmark existentes comparam métodos de preenchimento de lacunas
em conjuntos de dados fixos com um único modelo de ruído, reportando apenas
métricas médias globais
. Eles não estratificam o desempenho
por uma medida de complexidade local que pudesse prever a qualidade por recorte
antes de qualquer reconstrução ser tentada.

\medskip
\noindent\textbf{Contribuições.}
Este artigo apresenta as seguintes contribuições:

\begin{enumerate}
 \item Introduzimos um benchmark rigoroso de 15 métodos clássicos de
 preenchimento de lacunas abrangendo seis famílias algorítmicas, complementado
 por cinco arquiteturas de aprendizado profundo (autocodificador,
 autocodificador variacional, GAN, U-Net, transformador de visão), abrangendo
 quatro sensores de satélite, cinco métricas de qualidade e quatro níveis de
 ruído (\Cref{sec:methods}).

 \item Demonstramos que a \emph{entropia de Shannon local} em múltiplas escalas
 espaciais é um preditor significativo, corrigido pela FDR, da qualidade de
 reconstrução para todos os 15 métodos -- explicando até --\% da variância no
 \gls{PSNR} via regressão robusta (\Cref{sec:results-correlation}).

 \item Mapeamos a estrutura espacial do erro de reconstrução usando I de Moran
 e \gls{LISA}, revelando que os hotspots são consistentemente co-localizados
 com zonas de alta entropia (\Cref{sec:results-spatial}).

 \item Disponibilizamos um pipeline reprodutível de código aberto\footnote{%
 \url{https://github.com/meiazero/pdi-entropy-gapfilling}}
 que permite a replicação completa de todos os resultados reportados.
\end{enumerate}

O restante deste artigo está organizado da seguinte forma.
A \Cref{sec:related} revisa os trabalhos relacionados.
A \Cref{sec:methods} descreve o conjunto de dados, o protocolo de simulação e
os métodos avaliados.
A \Cref{sec:experimental} detalha a configuração experimental e o arcabouço
estatístico.
A \Cref{sec:results} apresenta os principais resultados.
A \Cref{sec:discussion} discute as implicações e limitações.
A \Cref{sec:conclusion} conclui o trabalho.

% ============================================================
\section{Trabalhos Relacionados}
\label{sec:related}

\subsection{Preenchimento de Lacunas em Imagens de Satélite}

Os métodos de preenchimento de lacunas podem ser classificados em três famílias:
(i)~métodos \emph{espaciais}, que exploram informações de vizinhança de pixel
dentro de uma única imagem; (ii)~métodos \emph{temporais}, que utilizam séries
temporais de imagens para substituir observações corrompidas; e
(iii)~métodos \emph{espectrais}, que exploram correlações entre bandas.

Os métodos espaciais clássicos incluem interpolação por vizinho mais próximo,
bilinear e bicúbica ; métodos baseados em kernel,
como \gls{IDW} e splines \gls{RBF}; e krigagem
geoestatística .
Métodos baseados em transformada, como inpainting por \gls{DCT}
 e regularização no domínio wavelet
, promovem esparsidade no domínio da frequência.
A minimização de \gls{TV} impõe reconstruções
suaves por partes.
Os métodos baseados em recortes (\gls{NLM} , inpainting baseado em
exemplar ) exploram a auto-similaridade.
O \gls{CS} recupera sinais esparsos a partir de medições incompletas via
minimização $\ell_1$.

Os métodos temporais incluem o algoritmo iterativo \gls{DINEOF}
, interpolação temporal por spline
e análise harmônica de Fourier .
As abordagens de aprendizado profundo incluem autocodificadores convolucionais
, autocodificadores variacionais ,
GANs condicionais , e
modelos baseados em transformador .

\subsection{Complexidade de Cena e Dificuldade de Reconstrução}

A entropia de Shannon, originalmente proposta como medida de teoria da informação
, tem sido amplamente utilizada em análise de imagens
como proxy de complexidade de textura .
 discute sua relação com histogramas locais
e aleatoriedade espacial. Entretanto, seu papel como preditor de desempenho no
preenchimento de lacunas recebeu atenção limitada.

 notou que a qualidade de produtos fusionados degrada sobre
superfícies heterogêneas, mas não quantificou a relação com a entropia.
 propôs índices de qualidade para preenchimento de lacunas
sem incorporar medidas de complexidade local.
Até onde sabemos, nenhum estudo quantificou sistematicamente a relação
entropia-desempenho ao longo de 15 métodos, quatro sensores e múltiplas escalas
espaciais, com correção para múltiplas comparações.

\subsection{Autocorrelação Espacial de Erros de Reconstrução}

O I de Moran e o \gls{LISA}
são ferramentas padrão em estatística espacial para detecção de agrupamentos.
Sua aplicação a mapas de qualidade de imagem é bem estabelecida na avaliação de
pansharpening, mas incomum em benchmarks de
preenchimento de lacunas.

% ============================================================
\section{Dados e Métodos}
\label{sec:methods}

\subsection{Conjunto de Dados}
\label{sec:dataset}

O conjunto de dados foi construído por meio de um pipeline de quatro etapas:
(i)~aquisição de imagens do \gls{GEE},
(ii)~pré-processamento radiométrico,
(iii)~extração de recortes com simulação de lacunas e augmentação de ruído, e
(iv)~compilação em um corpus de benchmark unificado.

\paragraph{Área de estudo e aquisição de imagens.}
Coletamos cenas de satélite cobrindo a Esta\c{c}\~ao
Ecol\'ogica do Castanha\~o, uma unidade de conservação federal localizada no
Cear\'a, Brasil (aproximadamente $38,60$--$38,40^{\circ}$\,O,
$5,53$--$5,73^{\circ}$\,S), no período de outubro de 2023 a
outubro de 2025. As imagens foram consultadas e exportadas do \gls{GEE} como
GeoTIFFs otimizados para nuvem para o Google Drive. Cenas com cobertura de
nuvens superior a 15\% foram rejeitadas utilizando campos de qualidade
específicos de cada sensor
(\texttt{CLOUDY\_PIXEL\_PERCENTAGE} para o Sentinel-2;
\texttt{CLOUD\_COVER} para o Landsat-8/9), enquanto as cenas MODIS foram
filtradas via flags de banda QA. Quatro sensores foram utilizados:

\begin{itemize}
 \item \textbf{Sentinel-2}
 (S2\_SR\_HARMONIZED, Level-2A; resolução espacial de 10\,m;
 bandas B2, B3, B4, B8).
 \item \textbf{Landsat-8}
 (Coleção~2, Level-2; 30\,m; bandas SR\_B2, SR\_B3,
 SR\_B4, SR\_B5).
 \item \textbf{Landsat-9}
 (Coleção~2, Level-2; 30\,m; bandas SR\_B2, SR\_B3,
 SR\_B4, SR\_B5).
 \item \textbf{MODIS}
 (MOD09GA; resolução de exportação de 250\,m; bandas
 sur\_refl\_b01--b04).
\end{itemize}

\noindent
Todos os sensores fornecem, assim, quatro bandas espectrais cobrindo o espectro
visível (azul, verde, vermelho) e infravermelho próximo.

\paragraph{Pré-processamento radiométrico.}
Cada cena exportada foi convertida para reflectância de superfície utilizando
fatores de escala e offsets específicos de cada sensor
($1 \times 10^{-4}$ para Sentinel-2 e MODIS;
$2,75 \times 10^{-5}$ com offset aditivo de $-0,2$ para
Landsat-8/9).
O recorte percentil por banda nos percentis 1 e 99, seguido de
reescalonamento min-max, normalizou os valores de pixel para $[0,1]$.

\paragraph{Extração de recortes.}
Uma janela deslizante de $64 \times 64$ pixels com sobreposição de 50\%
(stride~32) foi aplicada a cada cena pré-processada.
Recortes com mais de 10\% dos pixels sinalizados como sem dados ou com média
próxima de zero ($<10^{-6}$) foram descartados.
Uma seleção aleatória determinística de 10\% das cenas disponíveis por
sensor controlou o tamanho do corpus.
Os recortes foram armazenados como arquivos GeoTIFF preservando o sistema de
referência de coordenadas original e a transformação afim.

\paragraph{Divisões e compilação.}
O corpus foi estratificado em conjuntos de treinamento~(70\%),
validação~(15\%) e teste~(15\%) por cena de origem para
evitar vazamento espacial.
Os diretórios de benchmark por sensor foram então compilados em um conjunto de
dados unificado com metadados Parquet consolidados.
O corpus completo compreende 77\,916 recortes
(\Cref{tab:dataset-stats}).

\input{tables/dataset-stats}

\subsection{Protocolo de Simulação de Lacunas}
\label{sec:simulation}

Adotamos a notação a seguir ao longo do artigo.
Seja $\mathbf{y} \in [0,1]^{H \times W \times C}$ um recorte \emph{limpo}
(livre de nuvens), $\mathbf{z} \in \{0,1\}^{H \times W}$ uma máscara binária
de lacuna ($z_{hw} = 1$ indica um pixel ausente), e
$\mathbf{x}$ a observação \emph{degradada} (corrompida por lacunas).
Simulamos lacunas utilizando máscaras sintéticas semelhantes a nuvens.
Para cada cena de origem, uma máscara binária é gerada pela sobreposição de
3--8 blobs Gaussianos posicionados aleatoriamente com raios aleatórios,
suavizados por um filtro Gaussiano ($\sigma = 0,08\,s$, onde $s$ é o tamanho
do recorte em pixels). O campo suavizado é limiarizado no percentil 90 de modo
que aproximadamente 10\% dos pixels sejam designados como lacunas. Formalmente:
\[
 x_{hwc} =
 \begin{cases}
 y_{hwc} + \varepsilon_{hwc} & \text{se } z_{hw} = 0
 \quad \text{(observado)},\\
 0 & \text{se } z_{hw} = 1
 \quad \text{(lacuna)},
 \end{cases}
\]
onde $\varepsilon_{hwc} \sim \mathcal{N}(0, \sigma^2)$ modela o ruído do
sensor. Denotamos a imagem degradada sem ruído como $\mathbf{x}$ (i.e.\
$\sigma = 0$, $\mathrm{SNR} = \infty$) e as variantes com ruído como
$\mathbf{x}_{40\mathrm{dB}}$, $\mathbf{x}_{30\mathrm{dB}}$ e
$\mathbf{x}_{20\mathrm{dB}}$, correspondendo ao ruído Gaussiano aditivo a 40,
30 e 20\,dB de \gls{SNR}, respectivamente. Cada método de preenchimento de
lacunas recebe $(\mathbf{x}, \mathbf{z})$ e produz uma reconstrução
$\hat{\mathbf{y}}$; as métricas de qualidade são computadas entre
$\hat{\mathbf{y}}$ e $\mathbf{y}$ exclusivamente nos pixels de lacuna
$\mathcal{G} = \{(h,w) : z_{hw} = 1\}$.

\subsection{Entropia de Shannon Local}
\label{sec:entropy}

Para um recorte limpo $\mathbf{y}$, computamos a \emph{entropia de Shannon
local} em cada pixel $(h, w)$ dentro de uma janela deslizante $\Omega$ de
tamanho $s \times s$. As bandas espectrais são primeiro somadas em uma única
imagem em escala de cinza, redimensionadas para uint8 ($[0, 255]$), e o
histograma dentro da janela é computado. A entropia local é então definida
como:
\begin{equation*}
 H_s(h,w) = -\sum_{k=0}^{255} p_k \log_2 p_k,
\end{equation*}
onde $p_k$ denota a frequência normalizada do bin da imagem média de banda
redimensionada para uint8 dentro da vizinhança $\Omega_{hw}^{(s)}$.
Avaliamos três tamanhos de janela $s \in \{7, 15, 31\}$, gerando mapas de
entropia por pixel:
\begin{equation*}
 \mathbf{H}_7,\; \mathbf{H}_{15},\; \mathbf{H}_{31}
 \;\in\; \mathbb{R}^{H \times W}.
\end{equation*}

Para obter um descritor escalar único por recorte, calculamos a média dos
valores de entropia sobre o conjunto de pixels de lacuna $\mathcal{G}$:
\begin{equation*}
 \bar{H}_s
 = \frac{1}{\lvert\mathcal{G}\rvert}
   \sum_{(h,w)\,\in\,\mathcal{G}} H_s(h,w),
 \qquad
 \mathcal{G} = \bigl\{(h,w) : z_{hw} = 1\bigr\}.
\end{equation*}

\subsection{Métodos de Interpolação}
\label{sec:interpolation-methods}

Avaliamos 15 métodos clássicos organizados em seis categorias
(\Cref{tab:methods}). Todos os métodos operam independentemente em cada banda
espectral (canal a canal). Dado um recorte degradado $\mathbf{x}$ e a máscara
binária de lacuna $\mathbf{z}$, cada método reconstrói os pixels ausentes
$\hat{\mathbf{y}}$ preservando os observados. A seguir, descrevemos a lógica de
cada método aplicado a uma única banda espectral.

\input{tables/methods}

\subsubsection{Métodos Espaciais}

\paragraph{Vizinho mais próximo.}
Para cada pixel de lacuna, a transformada de distância Euclidiana da máscara de
pixels válidos é computada para identificar o pixel observado mais próximo. O
pixel de lacuna recebe então o valor desse vizinho mais próximo. O método é
livre de parâmetros e serve como a linha de base mais simples; preserva arestas
nítidas, mas introduz artefatos em blocos em regiões suaves.

\paragraph{Bilinear.}
Uma triangulação de Delaunay é construída a partir das coordenadas de todos os
pixels válidos. Cada pixel de lacuna cai dentro de um triângulo, e seu valor é
computado como uma média ponderada dos três vértices do triângulo usando
coordenadas baricêntricas. O resultado é uma superfície $C^0$-contínua. Pixels
de lacuna fora do casco convexo dos pixels válidos recaem sobre o vizinho mais
próximo.

\paragraph{Bicúbico.}
A mesma triangulação de Delaunay é utilizada, mas um esquema cúbico por partes
de Clough-Tocher substitui o interpolante linear. Isso produz uma superfície
$C^1$-contínua (continuamente diferenciável) que captura a curvatura local.
Assim como no bilinear, a extrapolação além do casco convexo recai sobre o
vizinho mais próximo.

\paragraph{Lanczos.}
Um algoritmo iterativo de Papoulis-Gerchberg é aplicado no domínio da
frequência. As lacunas são primeiro inicializadas com valores de vizinho mais
próximo, e a imagem é então repetidamente transformada para o domínio de
Fourier, onde uma janela Lanczos separável de ordem $a{=}3$ impõe limitação de
banda, e de volta ao domínio espacial. Após cada transformada inversa, os
valores de pixels conhecidos são restaurados exatamente (projeção de fidelidade
aos dados). O processo converge quando a mudança de raiz-média-quadrática nos
pixels de lacuna cai abaixo de $10^{-5}$ ou após 50 iterações.

\subsubsection{Métodos de Kernel}

\paragraph{Ponderação pelo Inverso da Distância (IDW).}
Cada pixel de lacuna é estimado como uma média ponderada de todos os pixels
válidos no recorte, com pesos inversamente proporcionais à distância Euclidiana
elevada a uma potência $p{=}2$ (método de Shepard):
\begin{equation*}
 \hat{X}_{hw} = \frac{\sum_i w_i \, X_i}{\sum_i w_i},
 \qquad
 w_i = \frac{1}{d_i^{\,p} + \varepsilon},
\end{equation*}
onde $d_i$ é a distância Euclidiana do pixel de lacuna ao $i$-ésimo pixel
válido e $\varepsilon = 10^{-10}$ previne divisão por zero.
O método é exato nos pixels observados e produz reconstruções suaves que decaem
para uma média global em grandes regiões de lacuna.

\paragraph{Função de Base Radial (RBF).}
As coordenadas dos pixels válidos servem como pontos de controle para um
interpolante RBF de spline de placa fina. O sistema resolve os pesos $\{w_k\}$
tais que a superfície interpolante é:
\begin{equation*}
 s(\mathbf{x})
 = \sum_k w_k \, \varphi\!\bigl(\|\mathbf{x} - \mathbf{x}_k\|\bigr),
 \qquad
 \varphi(r) = r^2 \ln r,
\end{equation*}
onde $\varphi$ é a função de base spline de placa fina. O interpolante é então
avaliado em todas as coordenadas de lacuna. Para manter o sistema linear
tratável, os pixels válidos são subamostrados para no máximo 5{,}000 pontos de
controle quando necessário.

\paragraph{Spline de placa fina.}
Este método é estruturalmente idêntico ao RBF com o kernel de spline de placa
fina, mas é configurado como uma entrada separada para enfatizar sua
interpretação de minimização de energia: minimiza a energia de curvatura
\begin{equation*}
 E(u) = \iint \bigl(\nabla^2 u\bigr)^2 \, \mathrm{d}A,
\end{equation*}
sujeito a restrições de interpolação nos pixels válidos.
A mesma estratégia de subamostragem se aplica.

\subsubsection{Método Geoestatístico}

\paragraph{Krigagem ordinária.}
Um semivariograma empírico é ajustado a partir de pares de pixels válidos usando
um modelo esférico com seis lags. A krigagem ordinária então prediz cada pixel
de lacuna como o Melhor Preditor Linear Não-Viesado (BLUP) derivado da
estrutura de correlação espacial. Para gerenciar o custo computacional, os
pixels válidos são limitados a 2.500 dentro de uma caixa delimitadora ao redor
da região de lacuna. Se o ajuste do variograma falhar, o método recai sobre
interpolação linear por griddata.

\subsubsection{Métodos no Domínio de Transformada}

\paragraph{DCT-ISTA.}
As lacunas são inicializadas com a média dos pixels válidos, e um Algoritmo de
Limiarização-Encolhimento Iterativo (ISTA) é aplicado no domínio da Transformada
Discreta de Cosseno 2D. Em cada iteração, a imagem é transformada, os
coeficientes são soft-limiarizados no nível $\lambda{=}0,05$, e a transformada
inversa é aplicada. Os valores de pixels conhecidos são então restaurados
(etapa de fidelidade aos dados). O laço termina após 50 iterações ou quando a
mudança de raiz-média-quadrática nos pixels de lacuna cai abaixo de $10^{-4}$.
A soft-limiarização promove esparsidade no domínio da frequência, efetivamente
regularizando a solução em direção à suavidade.

\paragraph{Wavelet-ISTA.}
O mesmo arcabouço ISTA é utilizado, mas a transformada esparsificante é uma
decomposição wavelet discreta em múltiplos níveis (wavelet Daubechies db4,
$L{=}3$ níveis). Apenas os coeficientes de detalhe são soft-limiarizados; a
sub-banda de aproximação é preservada. Após a reconstrução wavelet inversa, os
pixels conhecidos são restaurados. Esta abordagem penaliza detalhes de alta
frequência enquanto mantém a estrutura de grande escala.

\paragraph{Variação Total (TV).}
Um algoritmo primal-dual de Chambolle-Pock minimiza o funcional de variação
total:
\begin{equation*}
 \min_u \;\mathrm{TV}(u)
 + \frac{\lambda}{2}
   \sum_{(h,w) \notin \mathcal{G}}
   \bigl(u_{hw} - \tilde{X}_{hw}\bigr)^2,
\end{equation*}
onde $\lambda{=}0,1$. O termo de fidelidade aos dados aplica-se apenas aos
pixels observados (peso $= 1$); os pixels de lacuna recebem peso $= 0$. O
termo de TV impõe reconstruções suaves por partes via norma $\ell_1$ do
gradiente de imagem. O algoritmo executa 100 iterações fixas com tamanhos de
passo $\tau{=}\sigma{=}0,125$ e parâmetro de momento $\theta{=}1$.

\subsubsection{Métodos de Sensoriamento Compressivo ($\ell_1$)}

\paragraph{$\ell_1$-DCT.}
Um laço ISTA similar ao DCT-ISTA é aplicado com um orçamento de iteração maior
($K{=}100$) e tolerância de convergência mais restrita ($10^{-5}$). Em cada
etapa, todos os coeficientes DCT 2D são soft-limiarizados em $\lambda{=}0,05$,
promovendo esparsidade $\ell_1$ no domínio da frequência. Após a DCT inversa,
os valores de pixels conhecidos são impostos. O critério de parada mais estrito
permite que o algoritmo convirja com maior precisão, recuperando representações
espectrais esparsas a partir das observações incompletas.

\paragraph{$\ell_1$-Wavelet.}
Análogo ao $\ell_1$-DCT, mas a base esparsificante é a wavelet db4 em três
níveis de decomposição. Os coeficientes são achatados para um vetor 1D,
soft-limiarizados e reestruturados antes da reconstrução wavelet inversa. Os
pixels conhecidos são restaurados após cada iteração, e o laço termina em
$K{=}100$ iterações ou na convergência $\ell_2$ abaixo de $10^{-5}$.

\subsubsection{Métodos Baseados em Recortes}

\paragraph{Médias não-locais.}
As lacunas são primeiro preenchidas com a média local dos pixels válidos. Um
filtro de médias não-locais então refina a reconstrução, buscando recortes
similares dentro de um raio de 21 pixels e calculando a média com pesos
exponenciais proporcionais à similaridade entre recortes:
\begin{align}
  \label{eq:nlm}
  \hat{X}(\mathbf{x})
 &= \frac{1}{Z}
   \sum_{\mathbf{y}} w(\mathbf{x},\mathbf{y})\, u(\mathbf{y}), \\
 w(\mathbf{x},\mathbf{y})
 &\propto \exp\!\Bigl(
   -\frac{\|P_{\mathbf{x}} - P_{\mathbf{y}}\|^2}{h^2}
 \Bigr),
\end{align}
onde $Z$ é uma constante de normalização, $P_{\mathbf{x}}$ denota o recorte
$7 \times 7$ centrado em $\mathbf{x}$, e $h = 0,1\,\hat{\sigma}$ é a
intensidade do filtro escalada pelo nível de ruído estimado. Após a filtragem,
os valores de pixels conhecidos são restaurados exatamente.

\paragraph{Baseado em exemplar (biarmônico).}
A equação biarmônica
\begin{equation*}
 \nabla^4 u = 0
\end{equation*}
é resolvida dentro da região de lacuna, sujeita a condições de contorno na
interface entre pixels de lacuna e pixels válidos. Essa EDP de quarta ordem
minimiza a energia de curvatura da superfície interpolante, produzindo
reconstruções $C^1$-contínuas que estendem suavemente a estrutura circundante
para dentro da lacuna. O método é livre de parâmetros e se apoia na
implementação fornecida pelo scikit-image.

\subsection{Métodos de Aprendizado Profundo}
\label{sec:dl-methods}

Além dos 15 métodos clássicos, avaliamos cinco arquiteturas de \gls{DL}
treinadas de ponta a ponta na tarefa de preenchimento de lacunas. Como o foco
deste trabalho é entender como as representações aprendidas se comparam aos
esquemas de interpolação artesanal sob diferentes complexidades de cena,
descrevemos cada arquitetura em detalhes abaixo.

Todos os modelos recebem um tensor de entrada de cinco canais
$[\mathbf{x};\, \mathbf{z}] \in \mathbb{R}^{5 \times 64 \times 64}$
formado pela concatenação da imagem degradada $\mathbf{x}$ (quatro bandas
espectrais, com pixels de lacuna zerados) e da máscara binária de lacuna
$\mathbf{z}$. A saída é uma reconstrução de quatro canais
$\hat{\mathbf{y}} \in [0,1]^{4 \times 64 \times 64}$; os pixels observados
são preservados a partir da imagem limpa $\mathbf{y}$ via uma etapa de mesclagem
pós-hoc:
$\hat{\mathbf{y}}_{\mathrm{final}} = \mathbf{z} \odot \hat{\mathbf{y}}
+ (1 - \mathbf{z}) \odot \mathbf{y}$.
Todos os modelos são treinados com aritmética de precisão mista (passagens
forward em float16, atualizações de pesos em float32), recorte de gradiente na
norma unitária e parada antecipada na perda de validação. A função de perda,
salvo indicação em contrário, é o \gls{MSE} computado exclusivamente nos pixels
de lacuna $\mathcal{G}$:
\begin{equation*}
 \mathcal{L}_{\mathrm{MSE}}^{\mathcal{G}}
 = \frac{1}{|\mathcal{G}|\,C}
 \sum_{(h,w) \in \mathcal{G}} \sum_c
 (y_{hwc} - \hat{y}_{hwc})^2.
\end{equation*}
A \Cref{tab:dl-architectures} resume as cinco arquiteturas.

\input{tables/dl-architectures}

\subsubsection{Autocodificador Convolucional (AE)}

O \gls{AE} segue uma arquitetura codificador-decodificador simétrica com um
gargalo comprimido que força o modelo a aprender uma representação de baixa
dimensão da cena de entrada.

\paragraph{Codificador.}
O codificador compreende quatro blocos convolucionais, cada um consistindo em
uma convolução $4 \times 4$ com stride 2 e padding 1, seguida de normalização
em lote e ativação \gls{ReLU} ($f(x) = \max(0, x)$). A progressão de canais é
$5 \to 64 \to 128 \to 256 \to 512$, com as dimensões espaciais reduzindo pela
metade em cada estágio ($64^2 \to 32^2 \to 16^2 \to 8^2 \to 4^2$). Uma
convolução final $4 \times 4$ (sem stride) com \gls{ReLU} comprime o mapa de
características $512 \times 4 \times 4$ para um vetor de $512$ dimensões
($512 \times 1 \times 1$), gerando uma razão de compressão de
$5 \times 64^2 / 512 \approx 40\times$.

\paragraph{Decodificador.}
O decodificador espelha o codificador com cinco camadas de convolução transposta.
A primeira camada descompacta o gargalo de $512 \times 1 \times 1$ para
$512 \times 4 \times 4$ via convolução transposta $4 \times 4$ com normalização
em lote e \gls{ReLU}. As três camadas subsequentes fazem upsampling progressivo
com stride 2, reduzindo os canais através de $512 \to 256 \to 128 \to 64$,
cada uma com normalização em lote e \gls{ReLU}. A convolução transposta final
mapeia 64 canais para 4 canais de saída com ativação \emph{sigmoid}
($\sigma(x) = 1/(1 + e^{-x})$), limitando a saída a $[0,1]$.

\paragraph{Treinamento.}
Adam ($\mathrm{lr} = 10^{-3}$), 50 épocas, tamanho de lote 32, paciência de
parada antecipada de 10 épocas. A GapPixelLoss (\gls{MSE}) é computada apenas
nos pixels mascarados, garantindo que o modelo seja penalizado exclusivamente
pela sua reconstrução de dados ausentes.

\paragraph{Justificativa do projeto.}
O autocodificador serve como a linha de base profunda mais simples: deve
aprender um código latente comprimido a partir do qual o recorte completo pode
ser recuperado. A ausência de conexões residuais força toda a informação a
passar pelo gargalo de 512 dimensões, o que limita a reconstrução de detalhes
espaciais finos, mas fornece um forte efeito de regularização que previne
sobreajuste ao ruído.

\subsubsection{Autocodificador Variacional (VAE)}

O \gls{VAE} estende o autocodificador com um espaço latente estocástico que
impõe um prior probabilístico sobre as representações aprendidas, possibilitando
interpolação mais suave e reconstrução consciente de incerteza.

\paragraph{Codificador.}
O codificador convolucional é idêntico ao \gls{AE} (quatro blocos de convolução
$4 \times 4$ com normalização em lote e \gls{ReLU}, produzindo um mapa de
características $512 \times 4 \times 4$). O mapa de características é então
achatado para 8{,}192 dimensões e projetado através de duas camadas totalmente
conectadas paralelas para produzir dois vetores de $256$ dimensões: a média
$\boldsymbol{\mu}$ e a log-variância $\log \boldsymbol{\sigma}^2$.

\paragraph{Reparametrização.}
Durante o treinamento, uma amostra latente é extraída via o truque de
reparametrização: $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot
\boldsymbol{\varepsilon}$, onde
$\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.
Isso permite que os gradientes fluam pela etapa de amostragem estocástica.
Durante a inferência, a média $\boldsymbol{\mu}$ é utilizada diretamente para
saída determinística, evitando variância de reconstrução.

\paragraph{Decodificador.}
Uma camada linear projeta o vetor latente de 256 dimensões de volta para
$512 \times 4 \times 4 = 8{,}192$ dimensões, que é remodelado para o mapa de
características espacial. Quatro camadas de convolução transposta (cada uma com
normalização em lote e \gls{ReLU}) fazem upsampling através de
$512 \to 256 \to 128 \to 64$ canais. A convolução transposta final mapeia para
4 canais de saída com ativação sigmoid.

\paragraph{Função de perda.}
A perda total combina o \gls{MSE} dos pixels de lacuna com a divergência
\gls{KL} ponderada por $\beta{=}0,001$:
\begin{align}
 \mathcal{L}_{\mathrm{VAE}}
 &= \mathcal{L}_{\mathrm{MSE}}^{\mathcal{G}}
 + \beta \, D_{\mathrm{KL}}\bigl(
 q(\mathbf{z}|\mathbf{x}) \;\|\;
 p(\mathbf{z})\bigr), \nonumber \\
 D_{\mathrm{KL}}
 &= -\tfrac{1}{2}\sum_j \bigl(
 1 + \log\sigma_j^2
 - \mu_j^2 - \sigma_j^2\bigr).
\end{align}
O termo \gls{KL} atua como regularizador que previne o colapso do espaço latente
para uma estimativa pontual e encoraja interpolação suave entre reconstruções.
O valor baixo de $\beta$ prioriza a acurácia de reconstrução em detrimento da
regularidade do espaço latente, seguindo o arcabouço $\beta$-VAE.

\paragraph{Treinamento.}
Adam ($\mathrm{lr} = 10^{-3}$), 80 épocas, tamanho de lote 32, paciência de
parada antecipada de 10 épocas.

\subsubsection{Rede Adversarial Generativa (GAN)}

O modelo \gls{GAN} compreende um gerador U-Net e um discriminador PatchGAN
treinados em um arcabouço adversarial. O objetivo adversarial encoraja
reconstruções perceptualmente realistas além do que perdas pixel a pixel sozinhas
podem alcançar.

\paragraph{Gerador.}
O gerador segue o mesmo codificador de quatro estágios do \gls{AE}
($4 \times 4$ convoluções, stride 2, normalização em lote, \gls{ReLU};
canais $5 \to 64 \to 128 \to 256 \to 512$), mas substitui o gargalo de
compressão por um bloco de \emph{convolução dilatada}: duas convoluções
$3 \times 3$ com taxas de dilatação 2 e 4 (cada uma seguida de normalização em
lote e \gls{ReLU}). A dilatação expande o campo receptivo efetivo de
$3 \times 3$ para $5 \times 5$ e $9 \times 9$ respectivamente, permitindo que
o gargalo agregue contexto mais amplo sem aumentar a contagem de parâmetros.

Criticamente, o decodificador emprega \emph{conexões de salto}: mapas de
características de cada estágio do codificador são concatenados por canal com o
estágio correspondente do decodificador antes da convolução transposta. Os
estágios do decodificador processam assim $1024 \to 256$, $512 \to 128$,
$256 \to 64$ e $128 \to 4$ canais (canais concatenados de codificador +
decodificador em cada nível), cada um com normalização em lote e \gls{ReLU},
exceto a camada final que usa sigmoid. As conexões de salto preservam detalhes
espaciais de alta frequência que de outra forma seriam perdidos na compressão do
gargalo.

\paragraph{Discriminador.}
O discriminador PatchGAN recebe a imagem reconstruída (ou real) de quatro canais
como entrada. Consiste em quatro camadas convolucionais com kernels $4 \times 4$
e stride 2: a primeira camada ($4 \to 64$ canais) usa LeakyReLU com inclinação
negativa 0,2 e sem normalização; a segunda e terceira camadas ($64 \to 128$ e
$128 \to 256$) adicionam normalização em lote antes do LeakyReLU; a camada
final ($256 \to 1$) produz um mapa de probabilidade espacial $4 \times 4$ via
sigmoid. Cada elemento deste mapa classifica se a região de campo receptivo
correspondente da entrada é real ou gerada, em vez de produzir um julgamento
global único. Essa discriminação em nível de recorte fornece sinal de gradiente
mais rico para o gerador.

\paragraph{Funções de perda.}
A perda do gerador combina um termo de reconstrução $\ell_1$ (computado nos
pixels de lacuna apenas) e um termo adversarial:
\begin{align}
 \mathcal{L}_G = \lambda_{\ell_1}
 \mathcal{L}_{\ell_1}^{\mathcal{G}}
 + \lambda_{\mathrm{adv}} \,
 \mathrm{BCE}\bigl(D(\hat{\mathbf{y}}),\,
 \mathbf{1}\bigr),
\end{align}
com $\lambda_{\ell_1}{=}10$ e $\lambda_{\mathrm{adv}}{=}0,1$. O forte peso de
$\ell_1$ garante fidelidade espacial, enquanto o termo adversarial penaliza
texturas borradas ou implausíveis. A norma $\ell_1$ (vs.\ $\ell_2$) promove
reconstruções de arestas mais nítidas ao penalizar todos os erros linearmente
em vez de enfatizar grandes erros. A perda do discriminador é a \gls{BCE}
padrão em amostras reais e geradas:
\begin{align}
 \mathcal{L}_D = \mathrm{BCE}\bigl(D(\mathbf{y}),\, \mathbf{1}\bigr)
 + \mathrm{BCE}\bigl(D(\hat{\mathbf{y}}),\, \mathbf{0}\bigr).
\end{align}

\paragraph{Treinamento.}
Ambas as redes usam Adam ($\mathrm{lr} = 2 \times 10^{-4}$,
$\beta_1{=}0,5$, $\beta_2{=}0,999$) por 100 épocas com tamanho de lote 16 e
paciência de parada antecipada de 15 épocas (na perda de validação do gerador).
O $\beta_1$ reduzido estabiliza o treinamento adversarial ao amortecer o
momento. Na inferência, apenas o gerador é utilizado.

\subsubsection{U-Net}

A U-Net é o maior modelo do estudo, com aproximadamente 31 milhões de
parâmetros. Seu projeto combina aprendizado residual, conexões de salto e uma
função de ativação moderna para maximizar a capacidade de reconstrução.

\paragraph{Bloco de convolução residual.}
O bloco fundamental de construção consiste em duas convoluções $3 \times 3$
sequenciais (sem bias, para evitar redundância com a normalização em lote
subsequente) seguidas de normalização em lote e ativação \gls{GELU}. O
\gls{GELU} é definido como $\mathrm{GELU}(x) = x \, \Phi(x)$, onde $\Phi$ é
a CDF Gaussiana padrão; ao contrário do \gls{ReLU}, fornece um gate suave e
não-monotônico que demonstrou melhorar a dinâmica de treinamento em redes
profundas. Quando as dimensões de canal de entrada e saída diferem, uma
convolução de projeção $1 \times 1$ (com normalização em lote) é aplicada ao
caminho residual; caso contrário, um mapeamento identidade é usado. A conexão
residual ($\mathbf{y} = F(\mathbf{x}) + \mathbf{x}$) mitiga o problema de
gradiente que desaparece e permite o treinamento estável de redes mais profundas.

\paragraph{Codificador.}
Quatro estágios de codificador com progressão de canais $5 \to 64 \to 128 \to
256 \to 512$, cada um compreendendo um bloco de convolução residual seguido de
max-pooling $2 \times 2$. As dimensões espaciais reduzem à metade em cada
estágio: $64^2 \to 32^2 \to 16^2 \to 8^2 \to 4^2$.

\paragraph{Gargalo.}
Um único bloco de convolução residual com 1{,}024 canais operando em resolução
espacial $4 \times 4$.

\paragraph{Decodificador.}
Quatro estágios de decodificador, cada um realizando: (i)~convolução transposta
$2 \times 2$ com stride 2 para upsampling; (ii)~concatenação por canal com o
mapa de características do codificador conectado por salto na resolução
correspondente; (iii)~um bloco de convolução residual. A progressão de canais é
$1024 \to 512 \to 256 \to 128 \to 64$. Uma convolução $1 \times 1$ final com
sigmoid produz os quatro canais de saída.

\paragraph{Inicialização de pesos.}
Inicialização normal de Kaiming (modo fan-out, assumindo não-linearidade
\gls{ReLU}) para todas as camadas convolucionais e de convolução transposta;
pesos de normalização em lote inicializados como uns, biases como zeros.

\paragraph{Treinamento.}
AdamW ($\mathrm{lr} = 10^{-3}$, decaimento de peso $= 10^{-4}$) com
agendador de recomeço quente com annealing cosseno (período inicial $T_0{=}20$,
fator de duplicação $T_{\mathrm{mult}}{=}2$, taxa de aprendizado mínima
$10^{-6}$), 60 épocas, tamanho de lote 32, paciência de parada antecipada de
12 épocas. O decaimento de peso atua como regularizador $\ell_2$ em todos os
parâmetros (desacoplado da taxa de aprendizado adaptativa), prevenindo
sobreajuste no conjunto de dados de recortes relativamente pequeno. O
agendamento de recomeço quente permite que o otimizador escape de mínimos locais
rasos ao aumentar periodicamente a taxa de aprendizado.

\paragraph{Justificativa do projeto.}
A combinação de blocos residuais, conexões de salto e grande capacidade permite
que a U-Net capture tanto textura fina quanto estrutura espacial de grande
escala. As conexões de salto são particularmente importantes para o preenchimento
de lacunas: permitem que o decodificador acesse diretamente características do
codificador em alta resolução, contornando a compressão do gargalo que limita o
\gls{AE} e o \gls{VAE}.

\subsubsection{Transformador de Visão (ViT)}

O \gls{ViT} substitui a extração de características convolucionais por
autoatenção, fornecendo um \emph{campo receptivo global} desde a primeira
camada. Cada token atende a todos os outros tokens, permitindo que o modelo
explore dependências espaciais de longo alcance que os métodos convolucionais só
podem capturar por meio de empilhamento de muitas camadas. Isso é particularmente
relevante para grandes regiões de lacuna, onde o contexto distante pode carregar
mais informação do que os vizinhos locais.

\paragraph{Embedding de recortes.}
A entrada de cinco canais $64 \times 64$ é dividida em recortes não sobrepostos
de $8 \times 8$, gerando $N = 64$ tokens. Cada recorte é linearmente embutido
em um espaço de dimensão $d = 256$ via uma projeção convolucional
($8 \times 8$ kernel, stride 8). Embeddings posicionais aprendíveis
(inicializados a partir de $\mathcal{N}(0, 0,02)$) são somados elemento a
elemento para codificar a localização espacial, uma vez que o mecanismo de
autoatenção é inerentemente invariante a permutações.

\paragraph{Codificador Transformer.}
A sequência de tokens é processada por $L = 4$ blocos transformer, cada um
implementando uma arquitetura pré-norm:
\begin{enumerate}
 \item \textbf{Normalização de camada} aplicada antes de cada sub-camada
 (pré-norm melhora o fluxo de gradiente e a estabilidade de treinamento em
 comparação ao pós-norm).
 \item \textbf{Autoatenção multi-cabeça} com $h = 8$ cabeças (dimensão de
 cabeça $d_k = 256 / 8 = 32$). Para cada cabeça, queries, keys e values são
 computados via projeções lineares aprendidas; os pesos de atenção são
 $\mathrm{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.
 As oito cabeças são concatenadas e linearmente projetadas de volta para
 256 dimensões.
 \item \textbf{\gls{MLP}} com razão de expansão 4: duas camadas totalmente
 conectadas ($256 \to 1024 \to 256$) com ativação \gls{GELU} entre elas. A
 expansão cria uma representação intermediária de maior dimensão na qual
 interações de características não-lineares são computadas.
 \item \textbf{Conexões residuais} ao redor de ambas as sub-camadas de atenção
 e \gls{MLP}: $\mathbf{x} \leftarrow \mathbf{x} +
 \mathrm{Attn}(\mathrm{LN}(\mathbf{x}))$ e $\mathbf{x} \leftarrow
 \mathbf{x} + \mathrm{MLP}(\mathrm{LN}(\mathbf{x}))$.
\end{enumerate}

\paragraph{Cabeça de reconstrução.}
Uma normalização de camada final é aplicada à sequência de tokens codificados,
seguida de uma projeção linear de 256 para $8 \times 8 \times 4 = 256$
dimensões por token. Os tokens são remodelados e rearranjados de volta para uma
imagem $4 \times 64 \times 64$ via reorganização estilo pixel-shuffle, e então
passados pela ativação sigmoid.

\paragraph{Treinamento.}
AdamW ($\mathrm{lr} = 10^{-4}$, decaimento de peso $= 0,05$) com annealing
cosseno ($T_{\max}{=}100$, mínimo $10^{-6}$), 100 épocas, tamanho de lote 32,
paciência de parada antecipada de 15 épocas. A taxa de aprendizado mais baixa e
o maior decaimento de peso refletem práticas padrão para treinamento de
transformadores, onde a ausência de vieses indutivos (como equivariância de
translação em convoluções) requer regularização mais forte.

\subsection{Métricas de Qualidade}
\label{sec:metrics}

Todas as métricas são computadas exclusivamente nos pixels de lacuna
$\mathcal{G} = \{(h,w) : z_{hw}=1\}$ para evitar diluir o sinal com a
reconstrução (trivialmente perfeita) dos pixels observados.

\paragraph{PSNR.}
\begin{align}
 \mathrm{PSNR} &= 10 \log_{10}
 \!\left(\frac{1}{\mathrm{MSE}_{\mathcal{G}}}\right), \nonumber \\
 \mathrm{MSE}_{\mathcal{G}} &=
 \frac{1}{|\mathcal{G}|\,C}
 \sum_{(h,w)\in\mathcal{G}} \sum_c
 (y_{hwc} - \hat{y}_{hwc})^2.
\end{align}

\paragraph{SSIM.}
O mapa completo de \gls{SSIM} entre $\mathbf{y}$ e $\hat{\mathbf{y}}$ é
computado e calculada a média sobre $\mathcal{G}$.

\paragraph{RMSE.}
$\mathrm{RMSE}_{\mathcal{G}} = \sqrt{\mathrm{MSE}_{\mathcal{G}}}$.

\paragraph{SAM.}
O ângulo espectral entre os vetores espectrais de referência $\mathbf{y}$ e
reconstruído $\hat{\mathbf{y}}$ em cada pixel de lacuna, calculada a média sobre
$\mathcal{G}$.

\paragraph{ERGAS.}
Para preenchimento de lacunas ($h/l = 1$):
\begin{equation*}
 \mathrm{ERGAS} = 100
 \sqrt{\frac{1}{B'}
 \sum_{b \in \mathcal{B}^*}
 \left(\frac{\mathrm{RMSE}_b}
 {\bar{y}_{b,\mathcal{G}}}\right)^{\!2}},
\end{equation*}
onde $\mathcal{B}^*$ denota bandas com média não-nula e $B' = |\mathcal{B}^*|$.

\paragraph{Métricas locais.}
Para análise espacial, computamos também mapas de \gls{PSNR} e \gls{SSIM}
locais por janela deslizante por pixel, restritos às localizações de pixels de
lacuna, usando uma janela deslizante de 15$\times$15.

% ============================================================
\section{Configuração Experimental e Análise Estatística}
\label{sec:experimental}

\subsection{Delineamento Experimental}

Adotamos um delineamento completamente cruzado com 10 sementes aleatórias
$\times$ 4 níveis de ruído $\times$ 4 sensores de satélite $\times$ 15 métodos.
Para cada combinação, um conjunto fixo de até 150 recortes é amostrado por
sensor por semente. Um único conjunto de IDs de recortes é fixado por semente em
todos os níveis de ruído para garantir comparações emparelhadas.

Todos os experimentos são executados via um runner com checkpoint
(\texttt{scripts/run\_experiment.py}) com suporte a retomada a partir de
resultados parciais. Os resultados são gravados em um CSV nível-linha
(\texttt{raw\_results.csv}) com uma linha por
tupla (semente, nível de ruído, método, recorte).

\subsection{Análise de Correlação}
\label{sec:exp-correlation}

Computamos o $\rho$ de Spearman entre $\bar{H}_s$ e cada métrica para cada
tripla (método, janela de entropia, métrica).
Todos os $p$-valores são corrigidos para múltiplas comparações usando o
procedimento \gls{FDR} de Benjamini-Hochberg em $\alpha = 0,05$.
O $r$ de Pearson é reportado adicionalmente para comparação distribucional.

\subsection{Comparação de Métodos}
\label{sec:exp-comparison}

Testamos se os métodos diferem significativamente no \gls{PSNR} médio usando o
teste de Kruskal-Wallis (não-paramétrico; normalidade rejeitada neste tamanho
amostral). As comparações em pares usam a estatística $U$ de Mann-Whitney com
correção de Bonferroni. Os tamanhos de efeito são reportados como $\varepsilon^2$
(Kruskal-Wallis) e $\delta$ de Cliff (pares).

\subsection{Regressão Robusta}
\label{sec:exp-regression}

Para modelar conjuntamente a influência da entropia, do método e do nível de
ruído na qualidade de reconstrução, ajustamos o modelo linear robusto:
\begin{align}
 q_i = \beta_0
 &+ \sum_{s} \beta_s H_{s,i}
 + \sum_m \gamma_m \mathbb{1}[\mathrm{method}_i = m] \nonumber \\
 &+ \sum_n \delta_n \mathbb{1}[\mathrm{noise}_i = n]
 + \varepsilon_i,
\end{align}
onde $q_i$ denota o valor da métrica de qualidade (e.g.\ \gls{PSNR}) para a
observação $i$, usando o estimador-M Huber-T conforme implementado no
\gls{RLM} do \textsc{statsmodels}.
Os preditores são codificados com variáveis dummy (primeiro nível descartado).
Os fatores de inflação de variância (\gls{VIF}) são computados para os preditores
de entropia para diagnosticar multicolinearidade.
O Pseudo-$R^2_{\mathrm{adj}}$ é computado em relação aos resíduos de MQO para
comparabilidade.

\subsection{Autocorrelação Espacial}
\label{sec:exp-spatial}

Para um subconjunto representativo de recortes, computamos mapas de erro ao
quadrado por pixel e avaliamos o agrupamento espacial via:
\begin{itemize}
 \item \textbf{I de Moran}: autocorrelação espacial global usando pesos de
 contiguidade queen.
 \item \textbf{LISA}: rótulos de cluster local (HH, LL, HL, LH, NS) no nível
 de significância $\alpha = 0,05$ (999 permutações).
\end{itemize}

% ============================================================
\section{Resultados}
\label{sec:results}

\subsection{Comparação Quantitativa de Métodos}
\label{sec:results-global}

A \Cref{tab:psnr-method-noise} reporta o \gls{PSNR} médio
(IC de bootstrap 95\%) para todos os 15 métodos em cada nível de ruído.

\input{tables/psnr-method-noise}

A \Cref{tab:psnr-entropy-tercile} estratifica o \gls{PSNR} médio por tercil de
entropia (baixo / médio / alto) para cada método e janela de entropia.

\input{tables/psnr-entropy-tercile}

O teste de Kruskal-Wallis confirma diferenças significativas entre métodos
($H(14) = $ --, $p < 10^{-10}$,
$\varepsilon^2 = $ --). Os tamanhos de efeito $\delta$ de Cliff selecionados
par a par foram omitidos aqui por brevidade.

\subsection{Correlação Entropia-Desempenho}
\label{sec:results-correlation}

A \Cref{tab:spearman-heatmap} apresenta o $\rho$ de Spearman entre a entropia
multiescala e o \gls{PSNR} para métodos representativos (um por família).
A \Cref{fig:fig7_heatmap} visualiza os resultados como mapas de calor.

\input{tables/spearman-heatmap}

\subsection{Análise por Satélite}
\label{sec:results-satellite}

A \Cref{tab:psnr-satellite} desagrega o \gls{PSNR} médio por método e sensor.

\input{tables/psnr-satellite}

\subsection{Autocorrelação Espacial de Erros}
\label{sec:results-spatial}

A \Cref{fig:fig5_lisa} exibe mapas de cluster \gls{LISA} para recortes
representativos nos percentis de entropia baixo (P10), mediano (P50) e
alto (P90).

Os clusters HH (alto erro, rodeados por alto erro) se alinham fortemente com
regiões de alta entropia, enquanto os clusters LL (baixo erro) predominam em
áreas suaves e de baixa entropia.

\subsection{Comparação com Aprendizado Profundo}
\label{sec:results-dl}

A \Cref{tab:dl_comparison} reporta o \gls{PSNR} médio (IC de bootstrap 95\%)
para as cinco arquiteturas de \gls{DL} ao lado dos três principais métodos
clássicos em cada nível de ruído. Algumas observações se destacam.

\input{tables/dl-results}

\paragraph{Classificação geral.}
A U-Net alcança o maior \gls{PSNR} médio em todos os níveis de ruído, seguida
pelo gerador \gls{GAN} e pelo \gls{ViT}. O \gls{AE} e o \gls{VAE} -- que
carecem de conexões de salto -- ficam abaixo dos melhores métodos clássicos em
cenas de baixa entropia, mas fecham a diferença à medida que a entropia aumenta.

\paragraph{Efeito da entropia nos métodos de DL.}
Os modelos de \gls{DL} exibem uma correlação negativa entropia-\gls{PSNR}
similar à dos métodos clássicos, mas a inclinação é mais suave: a U-Net e o
\gls{GAN} mantêm \gls{PSNR} mais alto nos tercis de alta entropia em comparação
com os melhores métodos clássicos. Isso sugere que as representações aprendidas
compensam parcialmente a perda de informação em cenas complexas ao aproveitar
priors do conjunto de treinamento.

\paragraph{Robustez ao ruído.}
A U-Net e o \gls{ViT} mostram quedas menores de \gls{PSNR} de $\mathbf{x}$
para $\mathbf{x}_{20\mathrm{dB}}$ do que a maioria dos métodos clássicos,
indicando que a capacidade de denoising aprendida embutida no codificador
fornece robustez implícita ao ruído.

\paragraph{Comparação visual.}
A U-Net e o \gls{GAN} produzem reconstruções mais nítidas em cenas de alta
entropia, enquanto o \gls{AE} e o \gls{VAE} tendem a suavizar demais texturas
finas devido ao gargalo de informação. O \gls{ViT} captura bem a estrutura
global, mas pode introduzir ligeira fragmentação nas fronteiras dos tokens.

\paragraph{Análise de correlação entropia-DL.}
A correlação de Spearman entre a entropia multiescala e o \gls{PSNR} para
métodos de \gls{DL} é consistentemente negativa, confirmando que a relação
entropia-desempenho se generaliza para além da interpolação clássica. Entretanto,
a magnitude de $|\rho|$ é menor para a U-Net e o \gls{GAN}
($|\rho| \approx$ \TODO{--}) do que para o \gls{AE}
($|\rho| \approx$ \TODO{--}), sugerindo que as conexões de salto e o treinamento
adversarial mitigam a degradação induzida pela entropia.

% ---- Figuras ------------------------------------------------
\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig1_entropy_examples}
 \caption{%
 Mapas de entropia de Shannon local em três escalas espaciais (7$\times$7,
 15$\times$15, 31$\times$31 pixels) para recortes representativos do
 Sentinel-2 (linha superior) e Landsat-8 (linha inferior).
 Cores mais quentes indicam maior entropia (maior complexidade textural).
 }
 \label{fig:fig1_entropy}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig2_entropy_vs_psnr}
 \caption{%
 Gráficos de dispersão da entropia média do recorte (janela 7$\times$7)
 vs.\ \gls{PSNR} para todos os 15 métodos. Cada ponto representa uma avaliação
 (recorte, semente). Linhas de regressão e estatísticas de ajuste (equação,
 $r^2$, $r^2_{adj}$ e inclinação) são anotadas por painel.
 }
 \label{fig:fig2_scatter}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig3_psnr_by_entropy_bin}
 \caption{%
 Diagramas de caixa de \gls{PSNR} por método, agrupados por tercil de entropia
 (baixo / médio / alto). A degradação monotônica de qualidade do tercil baixo
 ao alto é consistente em todos os métodos.
 }
 \label{fig:fig3_boxplot_entropy}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig4_psnr_by_noise}
 \caption{%
 Diagramas de caixa de \gls{PSNR} por método agrupados por nível de ruído
 ($\mathrm{SNR} \in \{\infty, 40, 30, 20\}$\,dB).
 Os métodos no domínio de transformada (DCT, $\ell_1$-DCT) são os mais
 robustos ao ruído.
 }
 \label{fig:fig4_boxplot_noise}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig5_lisa_clusters}
 \caption{%
 Mapas de cluster \gls{LISA} (HH / LL / HL / LH / NS) sobrepostos em mapas de
 erro de reconstrução para recortes nos percentis de entropia 10, 50 e 90.
 O método de melhor classificação e um método de classificação intermediária
 são mostrados. Os clusters HH se co-localizam com zonas de alta entropia.
 }
 \label{fig:fig5_lisa}
\end{figure}

\begin{figure*}[t]
 \centering
 \includegraphics[width=0.98\textwidth]{figures/fig6_visual_examples_sentinel2}
 \caption{%
 Exemplos visuais de reconstrução (Sentinel-2; condição sem ruído) para
 recortes nos percentis de entropia baixo (P10), mediano (P50) e alto (P90).
 Colunas: referência limpa -- entrada degradada -- máscara de lacuna -- 4
 principais métodos por PSNR médio. Os valores de PSNR são anotados por painel.
 }
 \label{fig:fig6_visual}
\end{figure*}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig7_corr_heatmap_psnr}
 \caption{%
 Mapa de calor do $\rho$ de Spearman entre a entropia multiescala e o
 \gls{PSNR} para todos os 15 métodos. Células vermelhas indicam correlação
 negativa forte (maior entropia $\Rightarrow$ menor \gls{PSNR}).
 Asteriscos denotam significância FDR em $\alpha = 0,05$.
 }
 \label{fig:fig7_heatmap}
\end{figure}

% TODO: Adicionar geração de figuras específicas para DL (fig10--fig13) ao
% scripts/generate_figures.py assim que os resultados de DL estiverem em raw_results.csv.

% ============================================================
\section{Discussão}
\label{sec:discussion}

\subsection{Confirmação das Hipóteses}

\textbf{H1} (alta entropia $\Rightarrow$ menor qualidade) é confirmada.
A correlação de Spearman entre entropia e \gls{PSNR} é consistentemente
negativa, estatisticamente significativa após correção FDR, e robusta em todos
os 15 métodos, quatro sensores e três escalas de entropia. Os preditores de
entropia permanecem negativos após condicionamento na identidade do método e no
nível de ruído, indicando que a relação não é um artefato da composição dos
métodos ou do cenário de ruído.

\textbf{H2} (métodos geoestatísticos/de transformada melhor em áreas de baixa
entropia) é -- confirmada.
% TODO: Discutir a ordenação relativa no tercil de baixa entropia.

\textbf{H3} (autocorrelação espacial significativa) é confirmada.
I de Moran $> 0$ para todos os métodos em todos os recortes testados, com
$p < 0,001$ em todos os casos. A análise \gls{LISA} revela que os clusters de
erro HH se co-localizam com fronteiras texturais e núcleos de alta entropia.

\subsection{Métodos Clássicos vs.\ Aprendizado Profundo}

Os resultados de \gls{DL} (\Cref{sec:results-dl}) revelam um quadro nuançado.
A U-Net e o \gls{GAN} -- ambos empregando conexões de salto -- superam todos os
métodos clássicos em cenas de alta entropia, onde as representações aprendidas
podem explorar priors do conjunto de treinamento para alucinat textura
plausível. Em cenas de baixa entropia, entretanto, a vantagem diminui: os
métodos geoestatísticos e de domínio de transformada clássicos têm desempenho
comparável, com custo computacional significativamente menor.

O \gls{AE} e o \gls{VAE}, que carecem de conexões de salto e forçam toda a
informação por um gargalo estreito, têm dificuldades com detalhes espaciais
finos. Isso é consistente com a interpretação teórico-informacional: o gargalo
impõe um limite superior à informação mútua entre entrada e reconstrução, o
que é insuficiente para recortes de alta entropia. O \gls{ViT} alcança
desempenho competitivo por meio de seu campo receptivo global, mas apresenta
\gls{SSIM} marginalmente inferior ao da U-Net, provavelmente devido à
reconstrução independente de cada token $8 \times 8$ sem suavização espacial
explícita entre tokens.

A correlação entropia-\gls{PSNR} é negativa para todos os métodos de \gls{DL},
confirmando que a entropia de Shannon local permanece um preditor de
complexidade válido mesmo para abordagens aprendidas. Entretanto, a menor
magnitude de correlação para arquiteturas com conexões de salto (U-Net,
\gls{GAN}) sugere que esses modelos desacoplam parcialmente a qualidade de
reconstrução da complexidade de cena.

\subsection{Implicações Práticas}

A relação entropia-desempenho oferece um critério de pré-seleção fundamentado:
antes de aplicar qualquer método de preenchimento de lacunas, computar a
entropia local dos pixels de contexto disponíveis a partir de $\mathbf{y}$
(ou, na prática, da porção observada de $\mathbf{x}$). Se a entropia média da
região de lacuna exceder um limiar específico do sensor (calibrado a partir da
\Cref{tab:psnr-entropy-tercile}), considerar:
(i) métodos de \gls{DL} com conexões de salto (U-Net, \gls{GAN}), que
aproveitam priors aprendidos para reconstruir textura complexa;
(ii) métodos baseados em recortes (médias não-locais, baseado em exemplar), que
exploram a auto-similaridade global em vez da suavidade local; ou
(iii) métodos de \gls{CS}, que são inerentemente buscadores de esparsidade.

Para cenas de baixa entropia (suaves), todos os métodos -- incluindo as linhas
de base clássicas mais simples -- têm desempenho comparável, e a opção
computacionalmente mais barata (bilinear) pode ser suficiente. Esse resultado é
praticamente importante: sugere que o overhead computacional da inferência de
\gls{DL} só é justificado para cenas de alta complexidade.

\subsection{Limitações}

\begin{enumerate}
 \item \textbf{Lacunas simuladas.} Utilizamos máscaras de nuvem sintéticas
 derivadas de camadas de probabilidade de nuvem dos sensores. As bordas reais
 de nuvens podem ter estatísticas espaciais diferentes.
 \item \textbf{Informação temporal não explorada.} Métodos multi-temporais
 (spline temporal, DINEOF, Fourier) estão excluídos da análise primária porque
 requerem pilhas de séries temporais em vez de entradas de imagem única. Um
 benchmark temporal separado é necessário.
 \item \textbf{Escopo de treinamento do aprendizado profundo.} As cinco
 arquiteturas de DL são treinadas no mesmo conjunto de dados e avaliadas com
 as mesmas métricas, mas o ajuste de hiperparâmetros foi limitado a uma
 configuração única por modelo. Uma busca extensiva de arquitetura permanece
 como trabalho futuro.
 \item \textbf{Heterogeneidade de resolução espacial.} Os recortes MODIS
 (500\,m) têm estatísticas de textura fundamentalmente diferentes das do
 Sentinel-2 (10\,m). Limiares específicos por sensor podem ser necessários.
 \item \textbf{Entropia como preditor único.} A entropia não captura
 anisotropia, frequência dominante ou densidade de arestas. Trabalhos futuros
 poderiam incorporar descritores de textura complementares.
\end{enumerate}

% ============================================================
\section{Conclusão}
\label{sec:conclusion}

Apresentamos uma análise quantitativa em larga escala da relação entre a
entropia de Shannon local e o desempenho do preenchimento de lacunas em imagens
de satélite, abrangendo tanto interpolação clássica quanto abordagens de
\gls{DL}. Em mais de 77.916 recortes de teste de quatro sensores, 15 métodos
clássicos, cinco arquiteturas de \gls{DL} e quatro níveis de ruído,
demonstramos que:
\begin{itemize}
 \item A entropia de Shannon local é um preditor negativo estatisticamente
 significativo da qualidade de reconstrução (\gls{PSNR}, \gls{SSIM},
 \gls{RMSE}) para todos os 15 métodos clássicos e todas as cinco arquiteturas
 de \gls{DL}, em todas as escalas de entropia testadas.
 \item O efeito entropia-desempenho persiste após condicionamento na identidade
 do método, no nível de ruído e no sensor de satélite via regressão robusta.
 \item Os erros de reconstrução são autocorrelacionados espacialmente e se
 agrupam em zonas de alta entropia, conforme confirmado pelo I de Moran e pelo
 \gls{LISA}.
 \item Os modelos de \gls{DL} com conexões de salto (U-Net, \gls{GAN})
 mitigam parcialmente a degradação induzida pela entropia e superam todos os
 métodos clássicos em cenas de alta entropia, enquanto arquiteturas apenas com
 gargalo (\gls{AE}, \gls{VAE}) mostram desempenho comparável aos métodos
 clássicos.
 \item O custo computacional da inferência de \gls{DL} é justificado
 principalmente para cenas de alta complexidade; para recortes de baixa
 entropia, os métodos clássicos alcançam qualidade comparável a menor custo.
\end{itemize}

Esses resultados justificam a seleção adaptativa de métodos guiada pela
entropia -- roteando recortes de baixa entropia para métodos clássicos
eficientes e recortes de alta entropia para modelos de \gls{DL} -- e fornecem
um benchmark reprodutível para comparações futuras.
Todo o código, configurações e manifestos de recortes pré-processados são
disponibilizados em
\url{https://github.com/meiazero/pdi-entropy-gapfilling}.

% ============================================================

\begin{acknowledgements}
-- % Adicionar agradecimentos aqui.
\end{acknowledgements}

\begin{funding}
-- % Adicionar informações de financiamento aqui.
\end{funding}

\begin{contributions}
-- % Descrever as contribuições de cada autor aqui. Todos os autores leram e aprovaram a versão final do manuscrito.
\end{contributions}

\begin{interests}
Os autores declaram que não há conflito de interesses.
\end{interests}

\begin{materials}
Os conjuntos de dados gerados e analisados durante o presente estudo, juntamente
com todo o código-fonte, estão disponíveis em
\url{https://github.com/meiazero/pdi-entropy-gapfilling}.
\end{materials}


\bibliographystyle{apalike-sol}
\bibliography{refs}

\end{document}
