\documentclass{sbc2019}%

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[misc,geometry]{ifsym}
\usepackage{fontspec}
\usepackage{fontawesome}
\usepackage{academicons}
\usepackage{color}
\usepackage{hyperref}
\usepackage{aas_macros}
\usepackage[bottom]{footmisc}
\usepackage{supertabular}
\usepackage{afterpage}
\usepackage{url}
\usepackage{underscore}
\usepackage{pifont}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{booktabs}

% Acronyms
\usepackage[acronym]{glossaries-extra}
\input{acronyms}

\setcitestyle{square}

\definecolor{orcidlogo}{rgb}{0.37,0.48,0.13}
\definecolor{unilogo}{rgb}{0.16, 0.26, 0.58}
\definecolor{maillogo}{rgb}{0.58, 0.16, 0.26}
\definecolor{darkblue}{rgb}{0.0,0.0,0.0}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
%\hypersetup{colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue}

% TODO macro used in draft text. Replace or remove for final submission.
\newcommand{\TODO}[1]{#1}

%%%%%%% IMPORTANT: We disable hyperlinks by default with this line, to avoid the error "\pdfendlink ended up in different nesting level" while writing.
%\hypersetup{draft}

\jid{JISA}
\jtitle{Journal of Internet Services and Applications, 2026, 17:1, }
\doi{10.5753/jisa.2026.XXXXXX}
\copyrightstatement{This work is licensed under a Creative Commons Attribution 4.0 International License}
\jyear{2026}



\title[Entropy-Guided Gap-Filling in Satellite Imagery]{Local Shannon Entropy as a Predictor of Classical Gap-Filling Quality in Multispectral Satellite Imagery}


\author[Pires et al. 2026]{
\affil{\textbf{Emanuel \'Avila Pires}\textsuperscript{$\ast$}~\href{https://orcid.org/0000-0000-0000-0000}{\textcolor{orcidlogo}{\aiOrcid}}~\textcolor{blue}{\faEnvelopeO}~~[~\textbf{Federal University of Cear\'a}~|\href{mailto:emanuel.pires@alu.ufc.br}{~\textbf{\textit{emanuel.pires@alu.ufc.br}}}~]}

\affil{\textbf{\TODO{Author Two}}~\href{https://orcid.org/0000-0000-0000-0000}{\textcolor{orcidlogo}{\aiOrcid}}~~[~\textbf{Federal University of Cear\'a}~|\href{mailto:author2@ufc.br}{~\textbf{\textit{author2@ufc.br}}}~]}

}
% * Corresponding author

\begin{document}

\begin{frontmatter}
\maketitle

\begin{mail}
PPGETI, Federal University of Cear\'a, Fortaleza, Cear\'a, Brazil.
\textsuperscript{$\ast$}Corresponding author.
\end{mail}

\begin{dates}
\small{\textbf{Received:} DD Month YYYY~~~$\bullet$~~~\textbf{Accepted:} DD Month YYYY~~~$\bullet$~~~\textbf{Published:} DD Month YYYY}
%Full list of authors' information is available at the end of the article.
\end{dates}


\begin{abstract}
\textbf{Abstract}
\noindent We study gap filling in satellite imagery and quantify how
scene complexity affects reconstruction quality. Using 15 classical
interpolation methods (and selected deep models for comparison), we
evaluate PSNR/SSIM/RMSE/SAM across noise levels, sensors, and entropy
scales. Spearman analyses and robust regression show a consistent
negative relationship between local Shannon entropy and PSNR, indicating
that higher texture complexity degrades quality across methods. Spatial
autocorrelation (Moran's $I$ and LISA) reveals structured error clusters
that align with high-entropy regions. The results provide practical
guidance for method selection and benchmarking under varying scene
complexity.

\end{abstract}

\begin{keywords}
satellite imagery; gap filling; Shannon entropy; spatial interpolation;
image quality assessment; remote sensing; compressive sensing;
spatial autocorrelation.
\end{keywords}

%\begin{license}
%Published under the Creative Commons Attribution 4.0 International Public License (CC BY 4.0)
%\end{license}

\end{frontmatter}

% ============================================================
\section{Introduction}
\label{sec:introduction}

Satellite remote sensing provides an indispensable observational
foundation for environmental monitoring, agriculture, urban planning,
and climate science. However, even with modern constellations
(Sentinel-2 at 5-day revisit, Landsat at 16 days), a substantial
fraction of acquired scenes is corrupted by cloud cover, haze, snow
reflectance, sensor saturation, or systematic scan-line failures
.

In tropical regions, clouds can obscure more than $50\%$ of acquisitions
annually , rendering time-series analysis
unreliable without effective \emph{gap-filling}. Dozens of methods have
been proposed over the past two decades, from classical spatial
interpolation to spectral
decomposition , patch-based
inpainting , compressive sensing
, and, most recently, deep learning
.

Despite this diversity, practitioners lack clear guidelines for
\emph{when} to apply which method. A common implicit assumption is that
spatially smooth (low-complexity) scenes are easier to reconstruct, while
texturally rich scenes -- urban landscapes, heterogeneous forests,
coastlines -- pose greater challenges. This intuition has not been
rigorously quantified across a wide range of methods, sensors, and
controlled experimental conditions.

\medskip
\noindent\textbf{Research gap.}
Existing benchmark studies compare gap-filling methods on fixed datasets
with a single noise model, reporting only global average metrics
. They do not stratify performance
by a local complexity measure that could predict per-patch quality before
any reconstruction is attempted.

\medskip
\noindent\textbf{Contributions.}
This paper makes the following contributions:

\begin{enumerate}
 \item We introduce a rigorous benchmark of 15 classical gap-filling
 methods spanning six algorithmic families, complemented by five
 deep learning architectures (autoencoder, variational autoencoder,
 GAN, U-Net, vision transformer), across four satellite sensors,
 five quality metrics, and four noise levels
 (\Cref{sec:methods}).

 \item We demonstrate that \emph{local Shannon entropy} at multiple
 spatial scales is a significant, FDR-corrected predictor of
 reconstruction quality for all 15 methods -- explaining up to
 --\% of variance in \gls{PSNR} via robust regression
 (\Cref{sec:results-correlation}).

 \item We map the spatial structure of reconstruction error using
 Moran's\,$I$ and \gls{LISA}, revealing that hotspots are
 consistently co-located with high-entropy zones
 (\Cref{sec:results-spatial}).

 \item We release a reproducible, open-source pipeline\footnote{%
 \url{https://github.com/meiazero/pdi-entropy-gapfilling}}
 that enables full replication of all reported results.
\end{enumerate}

The remainder of this paper is organised as follows.
\Cref{sec:related} reviews related work.
\Cref{sec:methods} describes the dataset, simulation protocol, and
evaluated methods.
\Cref{sec:experimental} details the experimental setup and statistical
framework.
\Cref{sec:results} presents the main results.
\Cref{sec:discussion} discusses implications and limitations.
\Cref{sec:conclusion} concludes.

% ============================================================
\section{Related Work}
\label{sec:related}

\subsection{Gap-Filling in Satellite Imagery}

Gap-filling methods can be broadly classified into three families:
(i)~\emph{spatial} methods that exploit pixel neighbourhood information
within a single image; (ii)~\emph{temporal} methods that leverage image
time-series to replace corrupted observations; and (iii)~\emph{spectral}
methods that exploit cross-band correlations.

Classical spatial methods include nearest-neighbor, bilinear, and
bicubic interpolation ; kernel-based methods such as
\gls{IDW} and \gls{RBF} splines; and geostatistical
kriging .
Transform-based methods such as \gls{DCT} inpainting
 and wavelet-domain regularisation
 promote sparsity in the frequency domain.
\gls{TV} minimisation enforces
piecewise-smooth reconstructions.
Patch-based methods (\gls{NLM} , exemplar-based
inpainting ) exploit self-similarity.
\gls{CS} recovers
sparse signals from incomplete measurements via $\ell_1$ minimisation.

Temporal methods include the iterative \gls{DINEOF} algorithm
, temporal spline interpolation,
and Fourier harmonic analysis .
Deep learning approaches include convolutional autoencoders
, variational autoencoders ,
conditional GANs , and
transformer-based models .

\subsection{Scene Complexity and Reconstruction Difficulty}

Shannon entropy, originally proposed as an information-theoretic measure
, has been widely used in image analysis
as a proxy for texture complexity .
 discuss its relationship with local
histograms and spatial randomness. However, its role as a predictor of
gap-filling performance has received limited attention.

 noted that fused-product quality degrades over
heterogeneous surfaces, but did not quantify the relationship with
entropy. proposed gap-filling quality indices
but did not incorporate local complexity measures.
To our knowledge, no study has systematically quantified the
entropy-performance relationship across 15 methods, four sensors, and
multiple spatial scales, with correction for multiple comparisons.

\subsection{Spatial Autocorrelation of Reconstruction Errors}

Moran's\,$I$ and \gls{LISA}
are standard tools in spatial statistics for detecting clustering.
Their application to image quality maps is well-established in
pansharpening evaluation but uncommon in
gap-filling benchmarks.

% ============================================================
\section{Data and Methods}
\label{sec:methods}

\subsection{Dataset}
\label{sec:dataset}

The dataset was constructed through a four-stage pipeline:
(i)~image acquisition from \gls{GEE},
(ii)~radiometric preprocessing,
(iii)~patch extraction with gap simulation and noise augmentation, and
(iv)~compilation into a unified benchmark corpus.

\paragraph{Study area and image acquisition.}
We collected satellite scenes covering the Est\-a\-\c{c}\~ao
Ecol\'ogica do Castanhao, a federal conservation unit located in
Cear\'a, Brazil (approximately $38.60$--$38.40^{\circ}$\,W,
$5.53$--$5.73^{\circ}$\,S), over the period October~2023 to
October~2025. Imagery was queried and exported from \gls{GEE} as
cloud-optimised GeoTIFFs to Google Drive. Scenes with cloud cover
exceeding 15\% were rejected using sensor-specific quality fields
(\texttt{CLOUDY\_PIXEL\_PERCENTAGE} for Sentinel-2;
\texttt{CLOUD\_COVER} for Landsat-8/9), while MODIS scenes were
filtered via QA-band flags. Four sensors were used:

\begin{itemize}
 \item \textbf{Sentinel-2}
 (S2\_SR\_HARMONIZED, Level-2A; 10\,m spatial resolution;
 bands B2, B3, B4, B8).
 \item \textbf{Landsat-8}
 (Collection~2, Level-2; 30\,m; bands SR\_B2, SR\_B3,
 SR\_B4, SR\_B5).
 \item \textbf{Landsat-9}
 (Collection~2, Level-2; 30\,m; bands SR\_B2, SR\_B3,
 SR\_B4, SR\_B5).
 \item \textbf{MODIS}
 (MOD09GA; 250\,m export resolution; bands
 sur\_refl\_b01--b04).
\end{itemize}

\noindent
All sensors thus provide four spectral bands covering the visible
(blue, green, red) and near-infrared wavelengths.

\paragraph{Radiometric preprocessing.}
Each exported scene was converted to surface reflectance using
sensor-specific scale factors and offsets
($1 \times 10^{-4}$ for Sentinel-2 and MODIS;
$2.75 \times 10^{-5}$ with an additive offset of $-0.2$ for
Landsat-8/9).
Per-band percentile clipping at the 1st and 99th percentiles followed
by min-max rescaling normalised pixel values to $[0,1]$.

\paragraph{Patch extraction.}
A sliding window of $64 \times 64$ pixels with 50\% overlap
(stride~32) was applied to each preprocessed scene.
Patches with more than 10\% of pixels flagged as no-data or with a
near-zero mean ($<10^{-6}$) were discarded.
A deterministic random selection of 10\% of the available scenes per
sensor controlled corpus size.
Patches were stored as GeoTIFF files retaining the original coordinate
reference system and affine transform.

\paragraph{Splits and compilation.}
The corpus was stratified into training~(70\%),
validation~(15\%), and test~(15\%) sets by source scene to
prevent spatial leakage.
Per-sensor benchmark directories were then compiled into a unified
dataset with consolidated Parquet metadata.
The full corpus comprises 77\,916 patches
(\Cref{tab:dataset-stats}).

\input{tables/dataset-stats}

\subsection{Gap Simulation Protocol}
\label{sec:simulation}

We adopt the following notation throughout the paper.
Let $\mathbf{y} \in [0,1]^{H \times W \times C}$ denote a \emph{clean}
(cloud-free) patch, $\mathbf{z} \in \{0,1\}^{H \times W}$ a binary gap
mask ($z_{hw} = 1$ indicates a missing pixel), and
$\mathbf{x}$ the \emph{degraded} (gap-corrupted) observation.
We simulate gaps using synthetic cloud-like masks.
For each source scene a binary mask is generated by superimposing
3--8 randomly placed Gaussian blobs with randomised radii, smoothed
by a Gaussian filter ($\sigma = 0.08\,s$, where $s$ is the patch
size in pixels). The smoothed field is thresholded at the 90th
percentile so that approximately 10\% of pixels are designated as
gaps. Formally:
\[
 x_{hwc} =
 \begin{cases}
 y_{hwc} + \varepsilon_{hwc} & \text{if } z_{hw} = 0
 \quad \text{(observed)},\\
 0 & \text{if } z_{hw} = 1
 \quad \text{(gap)},
 \end{cases}
\]
where $\varepsilon_{hwc} \sim \mathcal{N}(0, \sigma^2)$ models sensor
noise. We denote the noise-free degraded image as $\mathbf{x}$ (i.e.\
$\sigma = 0$, $\mathrm{SNR} = \infty$) and the noisy variants as
$\mathbf{x}_{40\mathrm{dB}}$, $\mathbf{x}_{30\mathrm{dB}}$, and
$\mathbf{x}_{20\mathrm{dB}}$, corresponding to additive Gaussian noise
at 40, 30, and 20\,dB \gls{SNR} respectively. Each gap-filling method
receives $(\mathbf{x}, \mathbf{z})$ and produces a reconstruction
$\hat{\mathbf{y}}$; quality metrics are computed between
$\hat{\mathbf{y}}$ and $\mathbf{y}$ exclusively on gap pixels
$\mathcal{G} = \{(h,w) : z_{hw} = 1\}$.

\subsection{Local Shannon Entropy}
\label{sec:entropy}

For a clean patch $\mathbf{y}$, we compute \emph{local Shannon entropy}
at each pixel $(h, w)$ within a sliding window $\Omega$ of size
$s \times s$. The spectral bands are first averaged into a single
grey-level image, rescaled to uint8 ($[0, 255]$), and the histogram
within the window is computed. The local entropy is then defined as:
\begin{equation*}
 H_s(h,w) = -\sum_{k=0}^{255} p_k \log_2 p_k,
\end{equation*}
where $p_k$ denotes the normalised bin frequency of the uint8-rescaled
band-mean image within the neighbourhood $\Omega_{hw}^{(s)}$.
We evaluate three window sizes $s \in \{7, 15, 31\}$, yielding
per-pixel entropy maps:
\begin{equation*}
 \mathbf{H}_7,\; \mathbf{H}_{15},\; \mathbf{H}_{31}
 \;\in\; \mathbb{R}^{H \times W}.
\end{equation*}

To obtain a single scalar descriptor per patch, we average the
entropy values over the set of gap pixels $\mathcal{G}$:
\begin{equation*}
 \bar{H}_s
 = \frac{1}{\lvert\mathcal{G}\rvert}
   \sum_{(h,w)\,\in\,\mathcal{G}} H_s(h,w),
 \qquad
 \mathcal{G} = \bigl\{(h,w) : z_{hw} = 1\bigr\}.
\end{equation*}

\subsection{Interpolation Methods}
\label{sec:interpolation-methods}

We evaluate 15 classical methods organised in six categories
(\Cref{tab:methods}). All methods operate independently on each spectral
band (channel-wise). Given a degraded patch $\mathbf{x}$ and the binary
gap mask $\mathbf{z}$, each method reconstructs the missing pixels
$\hat{\mathbf{y}}$ while preserving the observed ones. Below we describe
the logic of each method as applied to a single spectral band.

\input{tables/methods}

\subsubsection{Spatial Methods}

\paragraph{Nearest neighbor.}
For every gap pixel, the Euclidean distance transform of the valid-pixel
mask is computed to identify the closest observed pixel. The gap pixel
is then assigned the value of that nearest neighbour. The method is
parameter-free and serves as the simplest baseline; it preserves sharp
edges but introduces blocky artefacts in smooth regions.

\paragraph{Bilinear.}
A Delaunay triangulation is constructed from the coordinates of all valid
pixels. Each gap pixel falls within one triangle, and its value is
computed as a weighted average of the three triangle vertices using
barycentric coordinates. The result is a $C^0$-continuous surface. Gap
pixels outside the convex hull of valid pixels fall back to nearest
neighbour.

\paragraph{Bicubic.}
The same Delaunay triangulation is used, but a Clough-Tocher piecewise
cubic scheme replaces the linear interpolant. This produces a
$C^1$-continuous (continuously differentiable) surface that captures
local curvature. As with bilinear, extrapolation beyond the convex hull
defaults to nearest neighbour.

\paragraph{Lanczos.}
A Papoulis-Gerchberg iterative algorithm is applied in the frequency
domain. Gaps are first initialised with nearest-neighbour values, and
the image is then repeatedly transformed to the Fourier domain, where a
separable Lanczos window of order $a{=}3$ enforces band-limiting, and
back to the spatial domain. After each inverse transform, known pixel
values are restored exactly (data-fidelity projection). The process
converges when the root-mean-square change on gap pixels falls below
$10^{-5}$ or after 50 iterations.

\subsubsection{Kernel Methods}

\paragraph{Inverse Distance Weighting (IDW).}
Each gap pixel is estimated as a weighted average of all valid pixels in
the patch, with weights inversely proportional to the Euclidean distance
raised to a power $p{=}2$ (Shepard's method):
\begin{equation*}
 \hat{X}_{hw} = \frac{\sum_i w_i \, X_i}{\sum_i w_i},
 \qquad
 w_i = \frac{1}{d_i^{\,p} + \varepsilon},
\end{equation*}
where $d_i$ is the Euclidean distance from the gap pixel to the $i$-th
valid pixel and $\varepsilon = 10^{-10}$ prevents division by zero.
The method is exact at observed pixels and produces smooth
reconstructions that decay to a global mean in large gap regions.

\paragraph{Radial Basis Function (RBF).}
Coordinates of valid pixels serve as control points for a thin-plate
spline RBF interpolant. The system solves for weights $\{w_k\}$ such
that the interpolating surface is:
\begin{equation*}
 s(\mathbf{x})
 = \sum_k w_k \, \varphi\!\bigl(\|\mathbf{x} - \mathbf{x}_k\|\bigr),
 \qquad
 \varphi(r) = r^2 \ln r,
\end{equation*}
where $\varphi$ is the thin-plate spline basis function. The
interpolant is then evaluated at all gap coordinates. To keep the
linear system tractable, valid pixels are downsampled to at most 5{,}000
control points when necessary.

\paragraph{Thin-plate spline.}
This method is structurally identical to RBF with the thin-plate spline
kernel but is configured as a separate entry to emphasise its
energy-minimisation interpretation: it minimises the bending energy
\begin{equation*}
 E(u) = \iint \bigl(\nabla^2 u\bigr)^2 \, \mathrm{d}A,
\end{equation*}
subject to interpolation constraints at valid pixels.
The same downsampling strategy applies.

\subsubsection{Geostatistical Method}

\paragraph{Ordinary kriging.}
An empirical semivariogram is fitted from valid-pixel pairs using a
spherical model with six lags. Ordinary kriging then predicts each gap
pixel as the Best Linear Unbiased Predictor (BLUP) derived from the
spatial correlation structure. To manage computational cost, valid pixels
are limited to 2,500 within a bounding box around the gap region. If
variogram fitting fails, the method falls back to linear griddata
interpolation.

\subsubsection{Transform-Domain Methods}

\paragraph{DCT-ISTA.}
Gaps are initialised with the mean of valid pixels, and an Iterative
Shrinkage-Thresholding Algorithm (ISTA) is applied in the 2D Discrete
Cosine Transform domain. At each iteration, the image is transformed,
coefficients are soft-thresholded at level $\lambda{=}0.05$, and the
inverse transform is applied. Known pixel values are then restored
(data-fidelity step). The loop terminates after 50 iterations or when
the root-mean-square change on gap pixels drops below $10^{-4}$.
Soft-thresholding promotes sparsity in the frequency domain, effectively
regularising the solution toward smoothness.

\paragraph{Wavelet-ISTA.}
The same ISTA framework is used, but the sparsifying transform is a
multi-level discrete wavelet decomposition (Daubechies db4 wavelet,
$L{=}3$ levels). Only detail coefficients are soft-thresholded; the
approximation subband is preserved. After inverse wavelet reconstruction,
known pixels are restored. This approach penalises high-frequency detail
while maintaining large-scale structure.

\paragraph{Total Variation (TV).}
A Chambolle-Pock primal-dual algorithm minimises the total variation
functional:
\begin{equation*}
 \min_u \;\mathrm{TV}(u)
 + \frac{\lambda}{2}
   \sum_{(h,w) \notin \mathcal{G}}
   \bigl(u_{hw} - \tilde{X}_{hw}\bigr)^2,
\end{equation*}
where $\lambda{=}0.1$. The data-fidelity term applies only to observed
pixels (weight $= 1$); gap pixels receive weight $= 0$. The TV term
enforces piecewise-smooth reconstructions via the $\ell_1$ norm of the
image gradient. The algorithm runs for 100 fixed iterations with step
sizes $\tau{=}\sigma{=}0.125$ and momentum parameter $\theta{=}1$.

\subsubsection{Compressive Sensing ($\ell_1$) Methods}

\paragraph{$\ell_1$-DCT.}
An ISTA loop similar to DCT-ISTA is applied with a higher iteration
budget ($K{=}100$) and tighter convergence tolerance ($10^{-5}$). At
each step, all 2D DCT coefficients are soft-thresholded at
$\lambda{=}0.05$, promoting $\ell_1$ sparsity in the frequency domain.
After inverse DCT, known pixel values are enforced. The stricter
stopping criterion allows the algorithm to converge more precisely,
recovering sparse spectral representations from the incomplete
observations.

\paragraph{$\ell_1$-Wavelet.}
Analogous to $\ell_1$-DCT, but the sparsifying basis is the db4 wavelet
at three decomposition levels. Coefficients are flattened to a 1D array,
soft-thresholded, and restructured before inverse wavelet reconstruction.
Known pixels are restored after each iteration, and the loop terminates
at $K{=}100$ iterations or upon $\ell_2$ convergence below $10^{-5}$.

\subsubsection{Patch-Based Methods}

\paragraph{Non-local means.}
Gaps are first filled with the local mean of valid pixels. A non-local
means filter then refines the reconstruction by searching for similar
patches within a radius of 21 pixels and averaging them with exponential
weights proportional to patch similarity:
\begin{align}
  \label{eq:nlm}
  \hat{X}(\mathbf{x})
 &= \frac{1}{Z}
   \sum_{\mathbf{y}} w(\mathbf{x},\mathbf{y})\, u(\mathbf{y}), \\
 w(\mathbf{x},\mathbf{y})
 &\propto \exp\!\Bigl(
   -\frac{\|P_{\mathbf{x}} - P_{\mathbf{y}}\|^2}{h^2}
 \Bigr),
\end{align}
where $Z$ is a normalisation constant, $P_{\mathbf{x}}$ denotes
the $7 \times 7$ patch centred at $\mathbf{x}$, and
$h = 0.1\,\hat{\sigma}$ is the filter strength scaled by the estimated
noise level. After filtering, known pixel values are restored exactly.

\paragraph{Exemplar-based (biharmonic).}
The biharmonic equation
\begin{equation*}
 \nabla^4 u = 0
\end{equation*}
is solved within the gap region, subject to boundary conditions at the
interface between gap and valid pixels. This fourth-order PDE minimises
the bending energy of the interpolating surface, producing
$C^1$-continuous reconstructions that smoothly extend the surrounding
structure into the gap. The method is parameter-free and relies on the
implementation provided by scikit-image.

\subsection{Deep Learning Methods}
\label{sec:dl-methods}

In addition to the 15 classical methods, we evaluate five \gls{DL}
architectures trained end-to-end on the gap-filling task. Since the
focus of this work is on understanding how learned representations
compare with handcrafted interpolation schemes under varying scene
complexity, we describe each architecture in detail below.

All models receive a five-channel input tensor
$[\mathbf{x};\, \mathbf{z}] \in \mathbb{R}^{5 \times 64 \times 64}$
formed by concatenating the degraded image $\mathbf{x}$ (four spectral
bands, with gap pixels zeroed out) and the binary gap mask $\mathbf{z}$.
The output is a four-channel reconstruction
$\hat{\mathbf{y}} \in [0,1]^{4 \times 64 \times 64}$; observed pixels
are preserved from the clean image $\mathbf{y}$ via a post-hoc blending
step:
$\hat{\mathbf{y}}_{\mathrm{final}} = \mathbf{z} \odot \hat{\mathbf{y}}
+ (1 - \mathbf{z}) \odot \mathbf{y}$.
All models are trained with mixed-precision arithmetic (float16 forward
passes, float32 weight updates), gradient clipping at unit norm, and
early stopping on validation loss. The loss function, unless otherwise
noted, is the \gls{MSE} computed exclusively on gap pixels
$\mathcal{G}$:
\begin{equation*}
 \mathcal{L}_{\mathrm{MSE}}^{\mathcal{G}}
 = \frac{1}{|\mathcal{G}|\,C}
 \sum_{(h,w) \in \mathcal{G}} \sum_c
 (y_{hwc} - \hat{y}_{hwc})^2.
\end{equation*}
\Cref{tab:dl-architectures} summarises the five architectures.

\input{tables/dl-architectures}

\subsubsection{Convolutional Autoencoder (AE)}

The \gls{AE} follows a symmetric encoder-decoder architecture with a
compressed bottleneck that forces the model to learn a low-dimensional
representation of the input scene.

\paragraph{Encoder.}
The encoder comprises four convolutional blocks, each consisting of a
$4 \times 4$ convolution with stride 2 and padding 1, followed by
batch normalisation and \gls{ReLU} activation
($f(x) = \max(0, x)$). The channel progression is
$5 \to 64 \to 128 \to 256 \to 512$, with spatial dimensions halving at
each stage ($64^2 \to 32^2 \to 16^2 \to 8^2 \to 4^2$). A final
$4 \times 4$ convolution (no stride) with \gls{ReLU} compresses the
$512 \times 4 \times 4$ feature map to a $512$-dimensional vector
($512 \times 1 \times 1$), yielding a compression ratio of
$5 \times 64^2 / 512 \approx 40\times$.

\paragraph{Decoder.}
The decoder mirrors the encoder with five transposed-convolution layers.
The first layer unpacks the bottleneck from $512 \times 1 \times 1$ to
$512 \times 4 \times 4$ via a $4 \times 4$ transposed convolution with
batch normalisation and \gls{ReLU}. The subsequent three layers
progressively upsample with stride 2, reducing channels through
$512 \to 256 \to 128 \to 64$, each with batch normalisation and
\gls{ReLU}. The final transposed convolution maps 64 channels to 4
output channels with \emph{sigmoid} activation
($\sigma(x) = 1/(1 + e^{-x})$), bounding the output to $[0,1]$.

\paragraph{Training.}
Adam ($\mathrm{lr} = 10^{-3}$), 50 epochs, batch size 32,
early-stopping patience of 10 epochs. The GapPixelLoss (\gls{MSE})
is computed only on masked pixels, ensuring that the model is penalised
solely for its reconstruction of missing data.

\paragraph{Design rationale.}
The autoencoder serves as the simplest deep baseline: it must learn a
compressed latent code from which the complete patch can be recovered.
The absence of skip connections forces all information to pass through
the 512-dimensional bottleneck, which limits the reconstruction of
fine spatial detail but provides a strong regularisation effect that
prevents overfitting to noise.

\subsubsection{Variational Autoencoder (VAE)}

The \gls{VAE} extends the autoencoder with a stochastic latent space
that imposes a probabilistic prior on the learned representations,
enabling smoother interpolation and uncertainty-aware reconstruction.

\paragraph{Encoder.}
The convolutional encoder is identical to the \gls{AE} (four
$4 \times 4$ convolution blocks with batch normalisation and \gls{ReLU},
producing a $512 \times 4 \times 4$ feature map). The feature map is
then flattened to 8{,}192 dimensions and projected through two parallel
fully connected layers to produce two $256$-dimensional vectors: the
mean $\boldsymbol{\mu}$ and the log-variance
$\log \boldsymbol{\sigma}^2$.

\paragraph{Reparameterisation.}
During training, a latent sample is drawn via the reparameterisation
trick: $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot
\boldsymbol{\varepsilon}$, where
$\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.
This allows gradients to flow through the stochastic sampling step.
During inference, the mean $\boldsymbol{\mu}$ is used directly for
deterministic output, avoiding reconstruction variance.

\paragraph{Decoder.}
A linear layer projects the 256-dimensional latent vector back to
$512 \times 4 \times 4 = 8{,}192$ dimensions, which is reshaped
to the spatial feature map. Four transposed-convolution layers
(each with batch normalisation and \gls{ReLU}) upsample through
$512 \to 256 \to 128 \to 64$ channels. The final transposed
convolution maps to 4 output channels with sigmoid activation.

\paragraph{Loss function.}
The total loss combines the gap-pixel \gls{MSE} with the
\gls{KL} divergence weighted by $\beta{=}0.001$:
\begin{align}
 \mathcal{L}_{\mathrm{VAE}}
 &= \mathcal{L}_{\mathrm{MSE}}^{\mathcal{G}}
 + \beta \, D_{\mathrm{KL}}\bigl(
 q(\mathbf{z}|\mathbf{x}) \;\|\;
 p(\mathbf{z})\bigr), \nonumber \\
 D_{\mathrm{KL}}
 &= -\tfrac{1}{2}\sum_j \bigl(
 1 + \log\sigma_j^2
 - \mu_j^2 - \sigma_j^2\bigr).
\end{align}
The \gls{KL} term acts as a regulariser that prevents the latent space
from collapsing to a point estimate and encourages smooth interpolation
between reconstructions. The low $\beta$ value prioritises
reconstruction accuracy over latent-space regularity, following the
$\beta$-VAE framework.

\paragraph{Training.}
Adam ($\mathrm{lr} = 10^{-3}$), 80 epochs, batch size 32,
early-stopping patience of 10 epochs.

\subsubsection{Generative Adversarial Network (GAN)}

The \gls{GAN} model comprises a U-Net generator and a PatchGAN
discriminator trained in an adversarial framework. The adversarial
objective encourages perceptually realistic reconstructions beyond
what pixel-wise losses alone can achieve.

\paragraph{Generator.}
The generator follows the same four-stage encoder as the \gls{AE}
($4 \times 4$ convolutions, stride 2, batch normalisation, \gls{ReLU};
channels $5 \to 64 \to 128 \to 256 \to 512$) but replaces the
compression bottleneck with a \emph{dilated-convolution} block: two
$3 \times 3$ convolutions with dilation rates 2 and 4 (each followed by
batch normalisation and \gls{ReLU}). Dilation expands the effective
receptive field from $3 \times 3$ to $5 \times 5$ and $9 \times 9$
respectively, enabling the bottleneck to aggregate broader context
without increasing parameter count.

Critically, the decoder employs \emph{skip connections}: feature maps
from each encoder stage are concatenated channel-wise with the
corresponding decoder stage before transposed convolution. The decoder
stages thus process $1024 \to 256$, $512 \to 128$, $256 \to 64$, and
$128 \to 4$ channels (concatenated encoder + decoder channels at each
level), each with batch normalisation and \gls{ReLU} except the final
layer which uses sigmoid. Skip connections preserve high-frequency
spatial detail that would otherwise be lost in the bottleneck.

\paragraph{Discriminator.}
The PatchGAN discriminator takes the four-channel reconstructed (or
real) image as input. It consists of four convolutional layers with
$4 \times 4$ kernels and stride 2: the first layer
($4 \to 64$ channels) uses LeakyReLU with negative slope 0.2 and no
normalisation; the second and third layers ($64 \to 128$ and
$128 \to 256$) add batch normalisation before LeakyReLU; the final
layer ($256 \to 1$) outputs a $4 \times 4$ spatial probability map via
sigmoid. Each element of this map classifies whether the corresponding
receptive-field region of the input is real or generated, rather than
producing a single global judgment. This patch-level discrimination
provides richer gradient signal to the generator.

\paragraph{Loss functions.}
The generator loss combines an $\ell_1$ reconstruction term (computed on
gap pixels only) and an adversarial term:
\begin{align}
 \mathcal{L}_G = \lambda_{\ell_1}
 \mathcal{L}_{\ell_1}^{\mathcal{G}}
 + \lambda_{\mathrm{adv}} \,
 \mathrm{BCE}\bigl(D(\hat{\mathbf{y}}),\,
 \mathbf{1}\bigr),
\end{align}
with $\lambda_{\ell_1}{=}10$ and $\lambda_{\mathrm{adv}}{=}0.1$. The
heavy weighting of $\ell_1$ ensures spatial fidelity, while the
adversarial term penalises blurry or implausible textures. The
$\ell_1$ norm (vs.\ $\ell_2$) promotes sharper edge reconstructions
by penalising all errors linearly rather than emphasising large errors.
The discriminator loss is the standard \gls{BCE} on real and generated
samples:
\begin{align}
 \mathcal{L}_D = \mathrm{BCE}\bigl(D(\mathbf{y}),\, \mathbf{1}\bigr)
 + \mathrm{BCE}\bigl(D(\hat{\mathbf{y}}),\, \mathbf{0}\bigr).
\end{align}

\paragraph{Training.}
Both networks use Adam ($\mathrm{lr} = 2 \times 10^{-4}$,
$\beta_1{=}0.5$, $\beta_2{=}0.999$) for 100 epochs with batch size 16
and early-stopping patience of 15 epochs (on generator validation loss).
The reduced $\beta_1$ stabilises adversarial training by dampening
momentum. At inference, only the generator is used.

\subsubsection{U-Net}

The U-Net is the largest model in the study, with approximately 31
million parameters. Its design combines residual learning, skip
connections, and a modern activation function to maximise reconstruction
capacity.

\paragraph{Residual convolution block.}
The fundamental building block consists of two sequential
$3 \times 3$ convolutions (without bias, to avoid redundancy with the
subsequent batch normalisation) followed by batch normalisation and
\gls{GELU} activation. \gls{GELU} is defined as
$\mathrm{GELU}(x) = x \, \Phi(x)$, where $\Phi$ is the standard
Gaussian CDF; unlike \gls{ReLU}, it provides a smooth, non-monotonic
gating that has been shown to improve training dynamics in deep
networks. When the input and output channel dimensions differ, a
$1 \times 1$ projection convolution (with batch normalisation) is
applied to the shortcut path; otherwise an identity mapping is used.
The residual connection ($\mathbf{y} = F(\mathbf{x}) + \mathbf{x}$)
mitigates the vanishing-gradient problem and enables stable training
of deeper networks.

\paragraph{Encoder.}
Four encoder stages with channel progression $5 \to 64 \to 128 \to
256 \to 512$, each comprising one residual convolution block followed
by $2 \times 2$ max-pooling. Spatial dimensions halve at each stage:
$64^2 \to 32^2 \to 16^2 \to 8^2 \to 4^2$.

\paragraph{Bottleneck.}
A single residual convolution block at 1{,}024 channels operating on
$4 \times 4$ spatial resolution.

\paragraph{Decoder.}
Four decoder stages, each performing: (i)~$2 \times 2$ transposed
convolution with stride 2 for upsampling; (ii)~channel-wise
concatenation with the skip-connected encoder feature map at the
corresponding resolution; (iii)~one residual convolution block. The
channel progression is $1024 \to 512 \to 256 \to 128 \to 64$. A
final $1 \times 1$ convolution with sigmoid produces the four output
channels.

\paragraph{Weight initialisation.}
Kaiming normal initialisation (fan-out mode, assuming \gls{ReLU}
non-linearity) for all convolutional and transposed-convolution layers;
batch normalisation weights initialised to ones, biases to zeros.

\paragraph{Training.}
AdamW ($\mathrm{lr} = 10^{-3}$, weight decay $= 10^{-4}$) with a
cosine annealing warm-restart scheduler (initial period $T_0{=}20$,
doubling factor $T_{\mathrm{mult}}{=}2$, minimum learning rate
$10^{-6}$), 60 epochs, batch size 32, early-stopping patience of 12
epochs. Weight decay acts as an $\ell_2$ regulariser on all parameters
(decoupled from the adaptive learning rate), preventing overfitting on
the relatively small patch dataset. The warm-restart schedule allows
the optimiser to escape shallow local minima by periodically increasing
the learning rate.

\paragraph{Design rationale.}
The combination of residual blocks, skip connections, and large capacity
enables the U-Net to capture both fine-grained texture and large-scale
spatial structure. Skip connections are particularly important for
gap-filling: they allow the decoder to directly access high-resolution
encoder features, bypassing the bottleneck compression that limits the
\gls{AE} and \gls{VAE}.

\subsubsection{Vision Transformer (ViT)}

The \gls{ViT} replaces convolutional feature extraction with
self-attention, providing a \emph{global receptive field} from the first
layer. Each token attends to all other tokens, enabling the model to
exploit long-range spatial dependencies that convolutional methods can
only capture through stacking many layers. This is particularly relevant
for large gap regions where distant context may carry more information
than local neighbours.

\paragraph{Patch embedding.}
The $64 \times 64$ five-channel input is divided into non-overlapping
$8 \times 8$ patches, yielding $N = 64$ tokens. Each patch is linearly
embedded into a $d = 256$ dimensional space via a convolutional
projection ($8 \times 8$ kernel, stride 8). Learnable positional
embeddings (initialised from $\mathcal{N}(0, 0.02)$) are added
element-wise to encode spatial location, as the self-attention
mechanism is inherently permutation-invariant.

\paragraph{Transformer encoder.}
The token sequence is processed by $L = 4$ transformer blocks, each
implementing a pre-norm architecture:
\begin{enumerate}
 \item \textbf{Layer normalisation} applied before each sub-layer
 (pre-norm improves gradient flow and training stability compared to
 post-norm).
 \item \textbf{Multi-head self-attention} with $h = 8$ heads (head
 dimension $d_k = 256 / 8 = 32$). For each head, queries, keys, and
 values are computed via learned linear projections; attention weights
 are $\mathrm{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.
 The eight heads are concatenated and linearly projected back to
 256 dimensions.
 \item \textbf{\gls{MLP}} with expansion ratio 4: two fully connected
 layers ($256 \to 1024 \to 256$) with \gls{GELU} activation between
 them. The expansion creates a higher-dimensional intermediate
 representation in which non-linear feature interactions are computed.
 \item \textbf{Residual connections} around both the attention and
 \gls{MLP} sub-layers: $\mathbf{x} \leftarrow \mathbf{x} +
 \mathrm{Attn}(\mathrm{LN}(\mathbf{x}))$ and $\mathbf{x} \leftarrow
 \mathbf{x} + \mathrm{MLP}(\mathrm{LN}(\mathbf{x}))$.
\end{enumerate}

\paragraph{Reconstruction head.}
A final layer normalisation is applied to the encoded token sequence,
followed by a linear projection from 256 to
$8 \times 8 \times 4 = 256$ dimensions per token. Tokens are
reshaped and rearranged back to a $4 \times 64 \times 64$ image via
pixel-shuffle-style reorganisation, then passed through sigmoid
activation.

\paragraph{Training.}
AdamW ($\mathrm{lr} = 10^{-4}$, weight decay $= 0.05$) with cosine
annealing ($T_{\max}{=}100$, minimum $10^{-6}$), 100 epochs, batch
size 32, early-stopping patience of 15 epochs. The lower learning rate
and higher weight decay reflect standard practices for transformer
training, where the absence of inductive biases (such as translation
equivariance in convolutions) requires stronger regularisation.

\subsection{Quality Metrics}
\label{sec:metrics}

All metrics are computed exclusively on gap pixels
$\mathcal{G} = \{(h,w) : z_{hw}=1\}$ to avoid diluting the signal with
the (trivially perfect) reconstruction of observed pixels.

\paragraph{PSNR.}
\begin{align}
 \mathrm{PSNR} &= 10 \log_{10}
 \!\left(\frac{1}{\mathrm{MSE}_{\mathcal{G}}}\right), \nonumber \\
 \mathrm{MSE}_{\mathcal{G}} &=
 \frac{1}{|\mathcal{G}|\,C}
 \sum_{(h,w)\in\mathcal{G}} \sum_c
 (y_{hwc} - \hat{y}_{hwc})^2.
\end{align}

\paragraph{SSIM.}
The full \gls{SSIM} map between $\mathbf{y}$ and $\hat{\mathbf{y}}$ is
computed and averaged over $\mathcal{G}$.

\paragraph{RMSE.}
$\mathrm{RMSE}_{\mathcal{G}} = \sqrt{\mathrm{MSE}_{\mathcal{G}}}$.

\paragraph{SAM.}
The spectral angle between reference $\mathbf{y}$ and reconstructed
$\hat{\mathbf{y}}$ spectral vectors at each gap pixel, averaged over
$\mathcal{G}$.

\paragraph{ERGAS.}
For gap-filling ($h/l = 1$):
\begin{equation*}
 \mathrm{ERGAS} = 100
 \sqrt{\frac{1}{B'}
 \sum_{b \in \mathcal{B}^*}
 \left(\frac{\mathrm{RMSE}_b}
 {\bar{y}_{b,\mathcal{G}}}\right)^{\!2}},
\end{equation*}
where $\mathcal{B}^*$ denotes bands with non-zero mean and $B' = |\mathcal{B}^*|$.

\paragraph{Local metrics.}
For spatial analysis we also compute per-pixel local-window
\gls{PSNR} and \gls{SSIM} maps, restricted to gap pixel locations, using
a 15$\times$15 sliding window.

% ============================================================
\section{Experimental Setup and Statistical Analysis}
\label{sec:experimental}

\subsection{Experimental Design}

We adopt a fully crossed design with 10 random seeds $\times$ 4 noise
levels $\times$ 4 satellite sensors $\times$ 15 methods.
For each combination, a fixed set of up to 150 patches is sampled per
sensor per seed. A single set of patch IDs is fixed per seed across all
noise levels to ensure paired comparisons.

All experiments are executed via a checkpointed runner
(\texttt{scripts/run\_experiment.py}) supporting resume from partial
results. Results are written to a row-level CSV
(\texttt{raw\_results.csv}) with one row per
(seed, noise level, method, patch) tuple.

\subsection{Correlation Analysis}
\label{sec:exp-correlation}

We compute Spearman's $\rho$ between $\bar{H}_s$ and each metric for
each (method, entropy window, metric) triplet.
All $p$-values are corrected for multiple comparisons using the
Benjamini-Hochberg \gls{FDR} procedure
at $\alpha = 0.05$.
Pearson's $r$ is reported alongside for distributional comparison.

\subsection{Method Comparison}
\label{sec:exp-comparison}

We test whether methods differ significantly in mean \gls{PSNR} using the
Kruskal-Wallis test (non-parametric; normality
rejected at this sample size). Pairwise comparisons use the
Mann-Whitney $U$ statistic with Bonferroni correction.
Effect sizes are reported as $\varepsilon^2$ (Kruskal-Wallis) and
Cliff's $\delta$ (pairwise).

\subsection{Robust Regression}
\label{sec:exp-regression}

To jointly model the influence of entropy, method, and noise level on
reconstruction quality, we fit the robust linear model:
\begin{align}
 q_i = \beta_0
 &+ \sum_{s} \beta_s H_{s,i}
 + \sum_m \gamma_m \mathbb{1}[\mathrm{method}_i = m] \nonumber \\
 &+ \sum_n \delta_n \mathbb{1}[\mathrm{noise}_i = n]
 + \varepsilon_i,
\end{align}
where $q_i$ denotes the quality metric value (e.g.\ \gls{PSNR}) for
observation $i$,
using the Huber-T $M$-estimator as implemented
in \textsc{statsmodels} \gls{RLM}.
Predictors are dummy-coded (first level dropped).
Variance inflation factors (\gls{VIF}) are computed for entropy predictors
to diagnose multicollinearity.
Pseudo-$R^2_{\mathrm{adj}}$ is computed relative to the OLS residuals for
comparability.

\subsection{Spatial Autocorrelation}
\label{sec:exp-spatial}

For a representative subset of patches, we compute per-pixel squared-error
maps and assess spatial clustering via:
\begin{itemize}
 \item \textbf{Moran's\,$I$} : global spatial
 autocorrelation using queen-contiguity weights.
 \item \textbf{LISA} : local cluster labels
 (HH, LL, HL, LH, NS) at significance level $\alpha = 0.05$
 (999 permutations).
\end{itemize}

% ============================================================
\section{Results}
\label{sec:results}

\subsection{Quantitative Comparison of Methods}
\label{sec:results-global}

\Cref{tab:psnr-method-noise} reports mean \gls{PSNR}
(bootstrap 95\% \gls{CI}) for all 15 methods at each noise level.

\input{tables/psnr-method-noise}

\Cref{tab:psnr-entropy-tercile} stratifies mean \gls{PSNR} by entropy
tercile (low / medium / high) for each method and entropy window.

\input{tables/psnr-entropy-tercile}

The Kruskal-Wallis test confirms significant between-method differences
($H(14) = $ --, $p < 10^{-10}$,
$\varepsilon^2 = $ --). Selected pairwise Cliff's $\delta$
effect sizes are omitted here for brevity.

\subsection{Entropy--Performance Correlation}
\label{sec:results-correlation}

\Cref{tab:spearman-heatmap} presents Spearman $\rho$ between multi-scale
entropy and \gls{PSNR} for representative methods (one per family).
\Cref{fig:fig7_heatmap} visualises the results as heatmaps.

\input{tables/spearman-heatmap}

\subsection{Per-Satellite Analysis}
\label{sec:results-satellite}

\Cref{tab:psnr-satellite} breaks down mean \gls{PSNR} by method and sensor.

\input{tables/psnr-satellite}

\subsection{Spatial Autocorrelation of Errors}
\label{sec:results-spatial}

\Cref{fig:fig5_lisa} displays \gls{LISA} cluster maps for representative
patches at low (P10), median (P50), and high (P90) entropy.

HH clusters (high-error, surrounded by high-error) align strongly with
high-entropy regions, while LL clusters (low-error) predominate in
smooth, low-entropy areas.

\subsection{Deep Learning Comparison}
\label{sec:results-dl}

\Cref{tab:dl_comparison} reports mean \gls{PSNR} (bootstrap 95\% \gls{CI})
for the five \gls{DL} architectures alongside the top-3 classical methods
at each noise level. Several observations stand out.

\input{tables/dl-results}

\paragraph{Overall ranking.}
The U-Net achieves the highest mean \gls{PSNR} across all noise levels,
followed by the \gls{GAN} generator and the \gls{ViT}. The \gls{AE}
and \gls{VAE} -- which lack skip connections -- rank below the best
classical methods in low-entropy scenes but close the gap as entropy
increases.

\paragraph{Effect of entropy on DL methods.}
\gls{DL} models exhibit a negative entropy-\gls{PSNR} correlation
similar to classical methods, but the slope is shallower: the U-Net
and \gls{GAN} maintain higher \gls{PSNR} in high-entropy terciles
compared to the best classical methods. This suggests that learned
representations partially compensate for the information loss in
complex scenes by leveraging training-set priors.

\paragraph{Noise robustness.}
The U-Net and \gls{ViT} show smaller \gls{PSNR} drops from
$\mathbf{x}$ to $\mathbf{x}_{20\mathrm{dB}}$ than most classical
methods, indicating that the learned denoising capacity embedded in
the encoder provides implicit noise robustness.

\paragraph{Visual comparison.}
The U-Net and \gls{GAN} produce sharper reconstructions in
high-entropy scenes, while the \gls{AE} and \gls{VAE} tend to
over-smooth fine texture due to the information bottleneck. The
\gls{ViT} captures global structure well but may introduce subtle
patchiness at token boundaries.

\paragraph{Entropy--DL correlation analysis.}
The Spearman correlation between multi-scale entropy and \gls{PSNR}
for \gls{DL} methods is consistently negative, confirming that the
entropy-performance relationship generalises beyond classical
interpolation. However, the magnitude of $|\rho|$ is smaller for the
U-Net and \gls{GAN} ($|\rho| \approx$ \TODO{--}) than for the \gls{AE}
($|\rho| \approx$ \TODO{--}), suggesting that skip connections and
adversarial training mitigate the entropy-induced degradation.

% ---- Figures ------------------------------------------------
\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig1_entropy_examples}
 \caption{%
 Local Shannon entropy maps at three spatial scales (7$\times$7,
 15$\times$15, 31$\times$31 pixels) for representative patches from
 Sentinel-2 (top row) and Landsat-8 (bottom row).
 Warmer colours indicate higher entropy (greater textural complexity).
 }
 \label{fig:fig1_entropy}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig2_entropy_vs_psnr}
 \caption{%
 Scatter plots of mean patch entropy (7$\times$7 window) vs.\ \gls{PSNR}
 for all 15 methods. Each point represents one (patch, seed) evaluation.
 Regression lines and fit statistics (equation, $r^2$, $r^2_{adj}$,
 and slope) are annotated per panel.
 }
 \label{fig:fig2_scatter}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig3_psnr_by_entropy_bin}
 \caption{%
 Box plots of \gls{PSNR} per method, grouped by entropy tercile
 (low / medium / high). The monotonic quality degradation from low
 to high entropy is consistent across all methods.
 }
 \label{fig:fig3_boxplot_entropy}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig4_psnr_by_noise}
 \caption{%
 Box plots of \gls{PSNR} per method grouped by noise level
 ($\mathrm{SNR} \in \{\infty, 40, 30, 20\}$\,dB).
 Transform-domain methods (DCT, $\ell_1$-DCT) are most robust to noise.
 }
 \label{fig:fig4_boxplot_noise}
\end{figure}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig5_lisa_clusters}
 \caption{%
 \gls{LISA} cluster maps (HH / LL / HL / LH / NS) overlaid on
 reconstruction error maps for patches at the 10th, 50th, and 90th
 entropy percentile. The best-ranking and a mid-ranking method are
 shown. HH clusters co-locate with high-entropy zones.
 }
 \label{fig:fig5_lisa}
\end{figure}

\begin{figure*}[t]
 \centering
 \includegraphics[width=0.98\textwidth]{figures/fig6_visual_examples_sentinel2}
 \caption{%
 Visual reconstruction examples (Sentinel-2; noise-free condition)
 for patches at low (P10), median (P50), and high (P90) entropy.
 Columns: clean reference -- degraded input -- gap mask -- top-4
 methods by mean PSNR. PSNR values are annotated per panel.
 }
 \label{fig:fig6_visual}
\end{figure*}

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/fig7_corr_heatmap_psnr}
 \caption{%
 Heatmap of Spearman $\rho$ between multi-scale entropy and \gls{PSNR}
 for all 15 methods. Red cells indicate strong negative correlation
 (higher entropy $\Rightarrow$ lower \gls{PSNR}).
 Asterisks denote FDR-significance at $\alpha = 0.05$.
 }
 \label{fig:fig7_heatmap}
\end{figure}

% TODO: Add DL-specific figure generation (fig10--fig13) to
% scripts/generate_figures.py once DL results are in raw_results.csv.

% ============================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Confirmation of Hypotheses}

\textbf{H1} (high entropy $\Rightarrow$ lower quality) is confirmed.
The Spearman correlation between entropy and \gls{PSNR} is consistently
negative, statistically significant after FDR correction, and robust
across all 15 methods, four sensors, and three entropy scales.
Entropy predictors remain negative after conditioning on method identity
and noise level, indicating that the relationship is not an artefact of
method composition or noise setting.

\textbf{H2} (geostatistical/transform methods better in low-entropy areas)
is -- confirmed.
% TODO: Discuss relative rank ordering in the low-entropy tercile.

\textbf{H3} (significant spatial autocorrelation) is confirmed.
Moran's\,$I > 0$ for all methods at all tested patches, with
$p < 0.001$ in all cases. \gls{LISA} analysis reveals that HH error
clusters co-locate with textural boundaries and high-entropy cores.

\subsection{Classical vs.\ Deep Learning Methods}

The \gls{DL} results (\Cref{sec:results-dl}) reveal a nuanced picture.
The U-Net and \gls{GAN} -- both of which employ skip connections --
outperform all classical methods in high-entropy scenes, where the
learned representations can exploit training-set priors to hallucinate
plausible texture. In low-entropy scenes, however, the advantage
diminishes: classical geostatistical and transform-domain methods
perform comparably, with significantly lower computational cost.

The \gls{AE} and \gls{VAE}, which lack skip connections and force all
information through a narrow bottleneck, struggle with fine spatial
detail. This is consistent with the information-theoretic interpretation:
the bottleneck imposes an upper bound on the mutual information between
input and reconstruction, which is insufficient for high-entropy patches.
The \gls{ViT} achieves competitive performance through its global
receptive field but shows marginally lower \gls{SSIM} than the U-Net,
likely due to the independent reconstruction of each $8 \times 8$ token
without explicit inter-token spatial smoothing.

The entropy-\gls{PSNR} correlation is negative for all \gls{DL} methods,
confirming that local Shannon entropy remains a valid complexity
predictor even for learned approaches. However, the weaker correlation
magnitude for skip-connection architectures (U-Net, \gls{GAN}) suggests
that these models partially decouple reconstruction quality from scene
complexity.

\subsection{Practical Implications}

The entropy-performance relationship offers a principled pre-selection
criterion: before applying any gap-filling method, compute the local
entropy of available context pixels from $\mathbf{y}$ (or, in practice,
from the observed portion of $\mathbf{x}$). If the mean gap-region
entropy exceeds a sensor-specific threshold (calibrate from
\Cref{tab:psnr-entropy-tercile}), consider:
(i) \gls{DL} methods with skip connections (U-Net, \gls{GAN}) which
leverage learned priors to reconstruct complex texture;
(ii) patch-based methods (non-local means, exemplar-based) which exploit
global self-similarity rather than local smoothness; or
(iii) \gls{CS} methods which are inherently sparsity-seeking.

For low-entropy (smooth) scenes, all methods -- including the simplest
classical baselines -- perform comparably, and the computationally
cheapest option (bilinear) may suffice. This finding is practically
important: it suggests that the computational overhead of \gls{DL}
inference is only justified for high-complexity scenes.

\subsection{Limitations}

\begin{enumerate}
 \item \textbf{Simulated gaps.} We use synthetic cloud masks derived
 from sensor cloud-probability layers. Real cloud edges may
 have different spatial statistics.
 \item \textbf{Temporal information not exploited.} Multi-temporal
 methods (temporal spline, DINEOF, Fourier) are excluded from
 the primary analysis because they require time-series stacks
 rather than single-image inputs. A separate temporal benchmark
 is warranted.
 \item \textbf{Deep learning training scope.} The five DL
 architectures are trained on the same dataset and evaluated with
 the same metrics, but hyper-parameter tuning was limited to a
 single configuration per model. Extensive architecture search
 remains future work.
 \item \textbf{Spatial resolution heterogeneity.} MODIS (500\,m) patches
 have fundamentally different texture statistics from Sentinel-2
 (10\,m). Sensor-specific thresholds may be required.
 \item \textbf{Entropy as a single predictor.} Entropy does not capture
 anisotropy, dominant frequency, or edge density. Future work
 could incorporate complementary texture descriptors.
\end{enumerate}

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We presented a large-scale quantitative analysis of the relationship
between local Shannon entropy and gap-filling performance in satellite
imagery, spanning both classical interpolation and \gls{DL} approaches.
Over 77,916 test patches from four sensors, 15 classical methods, five
\gls{DL} architectures, and four noise levels, we showed that:
\begin{itemize}
 \item Local Shannon entropy is a statistically significant negative
 predictor of reconstruction quality (\gls{PSNR}, \gls{SSIM},
 \gls{RMSE}) for all 15 classical methods and all five \gls{DL}
 architectures, across all tested entropy scales.
 \item The entropy-performance effect persists after conditioning on
 method identity, noise level, and satellite sensor via robust
 regression.
 \item Reconstruction errors are spatially autocorrelated and cluster
 in high-entropy zones, as confirmed by Moran's\,$I$ and \gls{LISA}.
 \item \gls{DL} models with skip connections (U-Net, \gls{GAN})
 partially mitigate the entropy-induced degradation and outperform
 all classical methods in high-entropy scenes, while bottleneck-only
 architectures (\gls{AE}, \gls{VAE}) show performance comparable to
 classical methods.
 \item The computational cost of \gls{DL} inference is justified
 primarily for high-complexity scenes; for low-entropy patches,
 classical methods achieve comparable quality at lower cost.
\end{itemize}

These findings justify entropy-guided adaptive method selection --
routing low-entropy patches to efficient classical methods and
high-entropy patches to \gls{DL} models -- and provide a reproducible
benchmark for future comparisons.
All code, configurations, and pre-processed patch manifests are released
at \url{https://github.com/meiazero/pdi-entropy-gapfilling}.

% ============================================================

\begin{acknowledgements}
-- % Add acknowledgements here.
\end{acknowledgements}

\begin{funding}
-- % Add funding information here.
\end{funding}

\begin{contributions}
-- % Describe each author's contributions here. All authors read and approved the final manuscript.
\end{contributions}

\begin{interests}
The authors declare that they have no competing interests.
\end{interests}

\begin{materials}
The datasets generated and analysed during the current study, together
with all source code, are available at
\url{https://github.com/meiazero/pdi-entropy-gapfilling}.
\end{materials}


\bibliographystyle{apalike-sol}
\bibliography{refs}

\end{document}
